{
  "hash": "1965042d2de46a710adfcac2d22a7633",
  "result": {
    "markdown": "---\ntitle: 'Outlier/Anomaly Detection Blog'\ndate: '2023-11-28'\ncategories: ['Machine Learning', 'Anomaly Detection', 'DBSCAN']\ndescription: 'This post focuses on detecting anomalies/outliers in data sets and how to find and handle them.'\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n# Background\n\nThis blog post will explore how to find and detect anomalies in data sets. The data set that is used for this is a Walmart data set that provides store and sales information. The goal will be to detect outliers and whether they exist, and how to handle them. \n\n## Setup\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib's pyplot to display graphs and plot our data. Numpy will also be used for some array manipulation. Scikit learn (sklearn) libraries will also be imported for its metrics and models. We will use the metrics to see how accurate the model is and find the error it produces. The model libraries will be used to build the Linear Regression and also split the data.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info >= (3, 7)\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import mean_squared_error\n```\n:::\n\n\n### Exploring the Data\n\nLet's start by seeing what our data looks like.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Read data\ndata = pd.read_csv('walmart-data.csv')\nprint(data.shape)\ndata.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(421570, 17)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Store</th>\n      <th>Date</th>\n      <th>IsHoliday</th>\n      <th>Dept</th>\n      <th>Weekly_Sales</th>\n      <th>Temperature</th>\n      <th>Fuel_Price</th>\n      <th>MarkDown1</th>\n      <th>MarkDown2</th>\n      <th>MarkDown3</th>\n      <th>MarkDown4</th>\n      <th>MarkDown5</th>\n      <th>CPI</th>\n      <th>Unemployment</th>\n      <th>Type</th>\n      <th>Size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>24924.50</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>11737.12</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>0</td>\n      <td>17.0</td>\n      <td>13223.76</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>0</td>\n      <td>45.0</td>\n      <td>37.44</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>0</td>\n      <td>28.0</td>\n      <td>1085.29</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe dimensions of the data is (421570, 17). There were 16 main categories provided that gave information on Walmart. There are 421,570 data entries, so there is a lot of data that can be utilized. We want to see what really drives weekly sales, so that will be what to compare against for each other category.\n\n### Plotting the Data\n\nLet's plot the data to visualize what the distributions look like and see if we can draw any initial inferences from what we see. By looking at the plotted data, sometimes it can be fairly clear if outliers do exist.\n\nFirst, let's look at how CPI (consumer price index) at the time affects the weekly sales.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.xlabel('CPI')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['CPI'], data['Weekly_Sales'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<matplotlib.collections.PathCollection at 0x12384c910>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_Detection_files/figure-html/cell-4-output-2.png){width=619 height=429}\n:::\n:::\n\n\nThere do appear to be some outliers we can see immediately. The data point at the top left of the plot do not fit with the other clusters of data present. Generally all the points in to 0 to 140 range remain below 400,000, with the excpetion of the few way above. \n\nNow let's look at the temperature on the days of sales and whether that plays a role in how weekly sales are affected.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.xlabel('Temperature')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['Temperature'], data['Weekly_Sales'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<matplotlib.collections.PathCollection at 0x1238da250>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_Detection_files/figure-html/cell-5-output-2.png){width=619 height=429}\n:::\n:::\n\n\nGenerally, the temperature does not make too big of a difference, and normal days do have better sales than those with temperatures that are too cold or too hot. The 40 to 60 degrees range is ideal, but in this range we do see some outliers that are significantly higher up than all other data points.\n\nLet's take a look at how the unemployment rate at the time affected the weekly sales.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.xlabel('Unemployment Rate')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['Unemployment'], data['Weekly_Sales'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<matplotlib.collections.PathCollection at 0x1238e9ed0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_Detection_files/figure-html/cell-6-output-2.png){width=619 height=429}\n:::\n:::\n\n\nThe range of between 7 to 10 percent unemployment where sales were the highest. Again however, in the 8 to 10 percent range there do appear to be a few outliers that exist much higher than all other data points. \n\nNow let's see how the holidays drive weekly sales. A holiday is represented by 1, and a non-holiday is represented by 0.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.xlabel('Holiday')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['IsHoliday'], data['Weekly_Sales'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.collections.PathCollection at 0x1238cea90>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Anomaly_Detection_files/figure-html/cell-7-output-2.png){width=619 height=429}\n:::\n:::\n\n\nHolidays do seem to bring in more sales on average. For non-holidays, there do not really appear to be outliers, but for holidays there seems to be a few towards to upper end, but is it difficult to tell. \n\n\n### Prepare The Data\n\nNow that we have a better understanding of what the data looks like and what we are searching for, we can begin our analysis on the data set and find outliers. \n\nLet's create the data set and drop the categories we do not need to take into account for this.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nX = data[['IsHoliday', 'Dept', 'Weekly_Sales',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size']]\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IsHoliday</th>\n      <th>Dept</th>\n      <th>Weekly_Sales</th>\n      <th>Temperature</th>\n      <th>Fuel_Price</th>\n      <th>MarkDown1</th>\n      <th>MarkDown2</th>\n      <th>MarkDown3</th>\n      <th>MarkDown4</th>\n      <th>MarkDown5</th>\n      <th>CPI</th>\n      <th>Unemployment</th>\n      <th>Type</th>\n      <th>Size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>24924.50</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>26.0</td>\n      <td>11737.12</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>17.0</td>\n      <td>13223.76</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>45.0</td>\n      <td>37.44</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>28.0</td>\n      <td>1085.29</td>\n      <td>42.31</td>\n      <td>2.572</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.096358</td>\n      <td>8.106</td>\n      <td>3</td>\n      <td>151315</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### DBSCAN - Outlier Detection\n\nFor detecting anomalies/outliers, we will use Density-Based Spatial Clustering, or DBSCAN, which will group data points based on their density. It will identify regions with high-density, and classify points with noise(meaningless/irrelevant data) as outliers.\n\nTo begin the DBSCAN, we have to note 2 parameters: eps and min samples. Eps, or Epsilon, is the maximum distance we will allow to exist between two data points for them to be considered neighbors. min samples, or minimum samples, which is the number of points needed within the radius for a point to determine number of clusters.\n\nBecause we know our data is widespread and can hold a big range, we will set our eps to 1000 and provide it a min_samples of 4.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# DBSCAN\ndbscan = DBSCAN(eps = 1000, min_samples= 4)\ndbscan.fit(X)\ndbscan.labels_\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([   0,    0,    0, ..., 5893, 5893, 5893])\n```\n:::\n:::\n\n\nLet's see how many outliers the DBSCAN detected.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Outlier count\nlabels = dbscan.labels_\narr = np.array(labels)\nprint(\"Number of Outliers: \", np.count_nonzero(arr == -1))\nprint(\"Percentage of Outliers: \", (sum(labels == -1) / X.shape[0]) * 100, \"%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of Outliers:  38479\nPercentage of Outliers:  9.127547026591076 %\n```\n:::\n:::\n\n\nAbout 9 percent of the data in this data set is being classified as anomalies/outliers. \n\nLet's visualize how the data was clustered through a table.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX_copy = X.copy()\nX_copy.loc[:, 'Cluster'] = dbscan.labels_\nX_copy.groupby('Cluster').size().to_frame()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>Cluster</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>38479</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6381</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>117</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5892</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5893</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>5894</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5895</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5896</th>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5898 rows × 1 columns</p>\n</div>\n```\n:::\n:::\n\n\nThis DBSCAN created almost 6000 clusters, and marked 38,479 as outliers. We can conclude that this data set does have several anomalies/outliers.\n\n",
    "supporting": [
      "Anomaly_Detection_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}