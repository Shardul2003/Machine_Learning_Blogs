{
  "hash": "d88d0e88f216f1af675518b294dea412",
  "result": {
    "markdown": "---\ntitle: 'Clustering Blog'\ndate: '2023-11-28'\ncategories: ['Machine Learning', 'Clustering', 'KMeans', 'DBSCAN']\ndescription: 'This post looks at KMeans Clustering and an analysis of it.'\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n# Background\n\nThis blog post looks at Clustering, specifically KMeans Clustering to analyze a data set on customers at a mall. It will focus on data analysis and performing a K Means on the data. We will also perform a DBSCAN to better understand the data and how well clustering in general will work. \n\n## Setup\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib's pyplot to display graphs and plot our data. Numpy will also be used for array manipulation. Scikit learn (sklearn) libraries will also be imported for its KMeans Clustering and DBSCAN. We will also use a yellowbrick import for the elbow visualizer, which will help find with KMeans optimizations.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n```\n:::\n\n\n### Data\n\nLet's start by seeing what our data looks like.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Read data\ndata = pd.read_csv(\"Mall_Customers.csv\")\nprint(data.shape)\ndata.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(200, 5)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerID</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Annual Income (k$)</th>\n      <th>Spending Score (1-100)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Male</td>\n      <td>19</td>\n      <td>15</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Male</td>\n      <td>21</td>\n      <td>15</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Female</td>\n      <td>20</td>\n      <td>16</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Female</td>\n      <td>23</td>\n      <td>16</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Female</td>\n      <td>31</td>\n      <td>17</td>\n      <td>40</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe dimensions of the data is (200, 5), so it contains the columns customerID, Gender, Age, Annual Income, and Spending Score. There are 200 data points for each individual that we can use. The customerID field will be unnecessary for our purposes, as it does not provide any valuable information. We want to know how spending score is affected by these factors on individuals, so we will start by looking at this.\n\n### Plotting the Data\n\nTo gain a better understanding of the data and what we can use, we can visualize it by plotting the features provided.\n\nLet's plot Age against Spending Score and see what the scatter plot for this will look like.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter(data['Age'], data['Spending Score (1-100)'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<matplotlib.collections.PathCollection at 0x129554850>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-4-output-2.png){width=665 height=466}\n:::\n:::\n\n\nThe data appears to be spread out, and it is possible to visualize some potential clusters here. One inference is that as customers get older their spending scores fall. After age 40, the scores see a immediate drop and then plateau, as in they do not go above 60 again.\n\n\nLet's look at Annual Income against Spending Score now.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score')\nplt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<matplotlib.collections.PathCollection at 0x1295f9b90>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-5-output-2.png){width=665 height=467}\n:::\n:::\n\n\nHere we can really see clusters present. There appears to be 5 distinct clusters, roughly one at each corner and one to the center-left. This makes using the idea of using a Clustering algorithm much more grounded, and shows potential for meaningful results.\n\nLet's also look at Gender really quickly and see if that happens to play a role.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.xlabel('Gender')\nplt.ylabel('Spending Score')\nplt.scatter(data['Gender'], data['Spending Score (1-100)'], color='g')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<matplotlib.collections.PathCollection at 0x12965b4d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-6-output-2.png){width=665 height=466}\n:::\n:::\n\n\nA scatter plot is used to see the differences between spending scores in gender's. From the graph, it does not appear that much difference exists, so we can dive a little deeper to truly be certain.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata['Gender'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nGender\nFemale    112\nMale       88\nName: count, dtype: int64\n```\n:::\n:::\n\n\nThere are more Female customers in the data set than Male, which could mean something. Let's take a look at the mean and median spending scores for each gender to see if any significant differences exists.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Find Mean and Median Spending Scores for each Gender\nfemale_data = data[data['Gender'] == 'Female']\nfemale_avg = female_data['Spending Score (1-100)'].mean()\nfemale_med = female_data['Spending Score (1-100)'].median()\nprint(\"Female Spending Score Average: \", female_avg)\nprint(\"Female Spending Score Median: \", female_med)\n\nmale_data = data[data['Gender'] == 'Male']\nmale_avg = male_data['Spending Score (1-100)'].mean()\nmale_med = male_data['Spending Score (1-100)'].median()\nprint(\"Male Spending Score Average: \", male_avg)\nprint(\"Male Spending Score Median: \", male_med)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFemale Spending Score Average:  51.526785714285715\nFemale Spending Score Median:  50.0\nMale Spending Score Average:  48.51136363636363\nMale Spending Score Median:  50.0\n```\n:::\n:::\n\n\nThe average spending score for Female's is slightly higher, but not by very much. The median scores for both gender's are also the same. This indicates that Gender is not too big a factor on how spending score is affected. \n\nThis further strengthens the argument for using a Clustering algorithm, as clustering algorithms generally do not use non-numeric data.\n\n### Setting up Clustering\n\nNow that we know what to expect from our analysis and what data to look at, we can begin setting up our data to use for our model. We will be looking at Age and Annual Income and their effects on spending score. Let's first begin with Annual Income.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nX = data[['Annual Income (k$)', 'Spending Score (1-100)']]\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Annual Income (k$)</th>\n      <th>Spending Score (1-100)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17</td>\n      <td>40</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSince we are using a KMeans algorithm, we have to find the number of clusters that makes the most sense. The KMeans will default to 8 clusters if not specified, but it is important to find the optimal number of clusters so that the data produced is well detailed and valuable.\n\nTo do this, we will use the KElbowVisualizer that will help find the optimal number of clusters. We will use this method and pass in a range of clusters and calculate their inertias (essentially the distances of data points to their closest cluster center). Based on the visualization, we choose the point where the curve cuts, or forms an elbow, and use that.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Perform elbow method to find optimal number of clusters\nk_means = KMeans(random_state=42)\nviz = KElbowVisualizer(k_means, k=(1, 12))\nviz.fit(X)\nviz.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-10-output-1.png){width=742 height=486}\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'>\n```\n:::\n:::\n\n\nWe see that the elbow forms at k = 5, meaning 5 is the optimal number of clusters to use in this case. This matches with the inference we had when analyzing our data for the annual income scatter plot initially.\n\n\n### Performing Clustering - KMeans\n\nNow we can go through with actually performing our KMeans. We will perform the KMeans, then take the clusters formed and append the column to our data (a copy of our data so we do not manipulate the original set), so we know which cluster each data point belongs to. Then, we plot this on a scatter plot and also include the centroids (center of each cluster) to visualize how the data was grouped.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Use 5 clusters\nk5_means = KMeans(n_clusters=5, init=\"k-means++\")\nclusters = k5_means.fit(X)\n# print(clusters)\n\n\nX_2 = X.copy() # data[['Annual Income (k$)', 'Spending Score (1-100)']]\nX_2.loc[:, 'Cluster'] = clusters.labels_ # Add Cluster column to data set\n\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score')\nplt.scatter(X_2['Annual Income (k$)'], X_2['Spending Score (1-100)'], c=X_2['Cluster'], cmap='viridis', s=50, alpha=0.5)\n\ncentroids = k5_means.cluster_centers_\nprint(centroids)\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.plot(centroids_x, centroids_y, 'ro',markersize=16, alpha = 0.5, label='centroids')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[86.53846154 82.12820513]\n [88.2        17.11428571]\n [55.2962963  49.51851852]\n [25.72727273 79.36363636]\n [26.30434783 20.91304348]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-11-output-2.png){width=665 height=467}\n:::\n:::\n\n\nThis clustering matches what was expected, with 5 clusters and colors that indicate which cluster each point belongs to. The red circles indicate the centroids. The centroids seem valid in their placements amongst the data. We can also see their dimensions and (x,y) positions by printing the centroids array.\n\nLets now go through this process again, this time with Age.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nX = data[['Age', 'Spending Score (1-100)']]\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Spending Score (1-100)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31</td>\n      <td>40</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis is our data set for this iteration of the KMeans. Let's again try the KElbowVisualizer to find the optimal number of clusters.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Perform elbow method to find optimal number of clusters\nk_means = KMeans(random_state=42)\nviz = KElbowVisualizer(k_means, k=(2, 12))\nviz.fit(X)\nviz.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-13-output-1.png){width=750 height=486}\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n<Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'>\n```\n:::\n:::\n\n\nThis time, the elbow is present at 4 clusters, indicating that 4 clusters is likely the optimal number to use for our KMeans. Let's do the same process as before to perform our KMeans.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nk5_means = KMeans(n_clusters=4, init=\"k-means++\")\nclusters = k5_means.fit(X)\nprint(clusters)\n\n\nX_2 = X.copy() #data[['Age', 'Spending Score (1-100)']]\nX_2.loc[:, 'Cluster'] = clusters.labels_ # Add Cluster column to data set\n\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter(X_2['Age'], X_2['Spending Score (1-100)'], c=X_2['Cluster'], cmap='viridis', s=50, alpha=0.5)\n\ncentroids = k5_means.cluster_centers_\nprint(centroids)\n\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.plot(centroids_x, centroids_y, 'ro',markersize=16, alpha = 0.5, label='centroids')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKMeans(n_clusters=4)\n[[27.61702128 49.14893617]\n [30.1754386  82.35087719]\n [43.29166667 15.02083333]\n [55.70833333 48.22916667]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-14-output-2.png){width=665 height=466}\n:::\n:::\n\n\nIn this case, the clusters seem to work well, but there do appear to be a significant number of outliers. The centroids appear to be accurate to where the clusters are marked. but there are points within each cluster that are fairly far from their respective cluster's center. This indicates that clustering works well here, but outliers do seem to exist.\n\n### More Data Analysis - DBSCAN\n\nLet's further analyze the data we have. We do see that outliers exist in the data, and some points in each cluster seem to be significantly distant from their centers. \n\nTo do this, we will use Density-Based Spatial Clustering, or DBSCAN, which will group data points based on their density. It will also identify regions with high-density, and classify points with noise(meaningless or uninterpretable data) as outliers.\n\nTo begin the DBSCAN, we have to note 2 parameters: eps and min samples. Eps, or Epsilon, is the maximum distance we will allow to exist between two data points for them to be considered neighbors. min samples, or minimum samples, which is the number of points needed within the radius for a point to determine number of clusters. \n\n\nHowever, we first need to figure out what eps to use and how many min_samples to provide. Since we have visualized our data and know what range it could relatively be in, we can try those possible values using numpy to get our range of values.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nscan_X = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\neps = np.arange(7, 13, 0.5)\nmin_samples = range(1,6)\n```\n:::\n\n\nWe can now perform the DBSCAN on each value and store it. We will also find the silhouette score, which is a metric that determine how well the clustering is. It will tell us how well each clustering has done given a specific eps and min samples.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndb_outputs = []\n\nfor ms in min_samples:\n    for ep in eps:\n        labels = DBSCAN(eps=ep, min_samples=ms).fit(scan_X).labels_\n        sil = silhouette_score(scan_X, labels)\n        db_outputs.append((sil, ep, ms))\n```\n:::\n\n\nNow we will sort the data so that we find the eps and min_samples for the entry with the highest silhouette score.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nsil_score, eps, min_samples = sorted(db_outputs, key=lambda x:x[-1])[-1]\nprint(\"eps: \", eps)\nprint(\"min_samples: \", min_samples)\nprint(\"Best silhouette_score: \", sil_score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neps:  12.5\nmin_samples:  5\nBest silhouette_score:  0.23413283117329386\n```\n:::\n:::\n\n\nThe optimal eps is 12.5 and min samples is 5. We can now use this to do the planned DBSCAN.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# DBSCSAN\ndbscan = DBSCAN(eps=12.5, min_samples=5)\ndbscan.fit(scan_X)\ndbscan.labels_\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\narray([-1,  0, -1,  0,  0,  0, -1, -1, -1,  0, -1, -1,  1,  0, -1,  0,  0,\n        0,  0, -1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n        1,  0,  1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  2, -1,  2,  0,  2,  3,  2,  3,  2,  0,  2, -1,  2,\n        3,  2, -1,  2,  3,  2, -1,  2, -1,  2,  0,  2,  3,  2,  3,  2,  3,\n        2,  3,  2,  3,  2, -1,  2, -1,  2, -1,  2,  3,  2,  3,  2,  3,  2,\n        3,  2,  3,  2,  3,  2,  3,  2,  3,  2, -1,  2,  3,  2, -1,  2, -1,\n       -1, -1,  2, -1, -1, -1,  2, -1, -1, -1, -1, -1, -1])\n```\n:::\n:::\n\n\nThe labels show which cluster the data point is assigned to, and gives it that label. -1 indicates it is not clustered as it did not meet the constraints provided to the DBSCAN. These can represent the outliers in our data set. Let's take a look at how many there are.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\narr = np.array(dbscan.labels_)\nnp.count_nonzero(arr == -1)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n32\n```\n:::\n:::\n\n\nThe DBSCAN labeled 32 points as -1, so therefore 32 data points as outliers in our set. Let's get a table look of this to see how many data points where in each cluster.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nX_copy = scan_X.copy()\nX_copy.loc[:, 'Cluster'] = dbscan.labels_\nX_copy.groupby('Cluster').size().to_frame()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>Cluster</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>109</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe see that the dbscan made 4 clusters. Cluster 0 had 109 data points, cluster 1 had 7, cluster 2 had 34, and cluster 3 had 18. The remaining 32 points were considered outliers.\n\n",
    "supporting": [
      "Clustering_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}