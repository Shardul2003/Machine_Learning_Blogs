[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nProbability Blog\n\n\n\n\n\n\n\nMachine Learning\n\n\nProbability\n\n\nLogistic Regression\n\n\nConfusion Matrix\n\n\n\n\nThis post looks at an application of probability within Machine Learning through the analysis of a data set.\n\n\n\n\n\n\nNov 28, 2023\n\n\nShardul Dhongade\n\n\n\n\n\n\n  \n\n\n\n\nClassification Blog\n\n\n\n\n\n\n\nMachine Learning\n\n\nClassification\n\n\nRandom Forest\n\n\n\n\nThis post walks through data analysis using Classification techniques such as Random Forest.\n\n\n\n\n\n\nNov 28, 2023\n\n\nShardul Dhongade\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression Blog\n\n\n\n\n\n\n\nMachine Learning\n\n\nLinear Regression\n\n\n\n\nThis post covers a dataset I analyze using Linear Regression.\n\n\n\n\n\n\nNov 28, 2023\n\n\nShardul Dhongade\n\n\n\n\n\n\n  \n\n\n\n\nOutlier/Anomaly Detection Blog\n\n\n\n\n\n\n\nMachine Learning\n\n\nAnomaly Detection\n\n\nDBSCAN\n\n\n\n\nThis post focuses on detecting anomalies/outliers in data sets and how to find and handle them.\n\n\n\n\n\n\nNov 28, 2023\n\n\nShardul Dhongade\n\n\n\n\n\n\n  \n\n\n\n\nClustering Blog\n\n\n\n\n\n\n\nMachine Learning\n\n\nClustering\n\n\nKMeans\n\n\nDBSCAN\n\n\n\n\nThis post looks at KMeans Clustering and an analysis of it.\n\n\n\n\n\n\nNov 28, 2023\n\n\nShardul Dhongade\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Clustering_Blog_Post/Clustering.html",
    "href": "posts/Clustering_Blog_Post/Clustering.html",
    "title": "Clustering Blog",
    "section": "",
    "text": "This blog post looks at Clustering, specifically KMeans Clustering to analyze a data set on customers at a mall. It will focus on data analysis and performing a K Means on the data. We will also perform a DBSCAN to better understand the data and how well clustering in general will work.\n\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Numpy will also be used for array manipulation. Scikit learn (sklearn) libraries will also be imported for its KMeans Clustering and DBSCAN. We will also use a yellowbrick import for the elbow visualizer, which will help find with KMeans optimizations.\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"Mall_Customers.csv\")\nprint(data.shape)\ndata.head()\n\n(200, 5)\n\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\nThe dimensions of the data is (200, 5), so it contains the columns customerID, Gender, Age, Annual Income, and Spending Score. There are 200 data points for each individual that we can use. The customerID field will be unnecessary for our purposes, as it does not provide any valuable information. We want to know how spending score is affected by these factors on individuals, so we will start by looking at this.\n\n\n\nTo gain a better understanding of the data and what we can use, we can visualize it by plotting the features provided.\nLet’s plot Age against Spending Score and see what the scatter plot for this will look like.\n\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter(data['Age'], data['Spending Score (1-100)'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x129554850&gt;\n\n\n\n\n\nThe data appears to be spread out, and it is possible to visualize some potential clusters here. One inference is that as customers get older their spending scores fall. After age 40, the scores see a immediate drop and then plateau, as in they do not go above 60 again.\nLet’s look at Annual Income against Spending Score now.\n\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score')\nplt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1295f9b90&gt;\n\n\n\n\n\nHere we can really see clusters present. There appears to be 5 distinct clusters, roughly one at each corner and one to the center-left. This makes using the idea of using a Clustering algorithm much more grounded, and shows potential for meaningful results.\nLet’s also look at Gender really quickly and see if that happens to play a role.\n\nplt.xlabel('Gender')\nplt.ylabel('Spending Score')\nplt.scatter(data['Gender'], data['Spending Score (1-100)'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x12965b4d0&gt;\n\n\n\n\n\nA scatter plot is used to see the differences between spending scores in gender’s. From the graph, it does not appear that much difference exists, so we can dive a little deeper to truly be certain.\n\ndata['Gender'].value_counts()\n\nGender\nFemale    112\nMale       88\nName: count, dtype: int64\n\n\nThere are more Female customers in the data set than Male, which could mean something. Let’s take a look at the mean and median spending scores for each gender to see if any significant differences exists.\n\n# Find Mean and Median Spending Scores for each Gender\nfemale_data = data[data['Gender'] == 'Female']\nfemale_avg = female_data['Spending Score (1-100)'].mean()\nfemale_med = female_data['Spending Score (1-100)'].median()\nprint(\"Female Spending Score Average: \", female_avg)\nprint(\"Female Spending Score Median: \", female_med)\n\nmale_data = data[data['Gender'] == 'Male']\nmale_avg = male_data['Spending Score (1-100)'].mean()\nmale_med = male_data['Spending Score (1-100)'].median()\nprint(\"Male Spending Score Average: \", male_avg)\nprint(\"Male Spending Score Median: \", male_med)\n\nFemale Spending Score Average:  51.526785714285715\nFemale Spending Score Median:  50.0\nMale Spending Score Average:  48.51136363636363\nMale Spending Score Median:  50.0\n\n\nThe average spending score for Female’s is slightly higher, but not by very much. The median scores for both gender’s are also the same. This indicates that Gender is not too big a factor on how spending score is affected.\nThis further strengthens the argument for using a Clustering algorithm, as clustering algorithms generally do not use non-numeric data.\n\n\n\nNow that we know what to expect from our analysis and what data to look at, we can begin setting up our data to use for our model. We will be looking at Age and Annual Income and their effects on spending score. Let’s first begin with Annual Income.\n\nX = data[['Annual Income (k$)', 'Spending Score (1-100)']]\nX.head()\n\n\n\n\n\n\n\n\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n15\n39\n\n\n1\n15\n81\n\n\n2\n16\n6\n\n\n3\n16\n77\n\n\n4\n17\n40\n\n\n\n\n\n\n\nSince we are using a KMeans algorithm, we have to find the number of clusters that makes the most sense. The KMeans will default to 8 clusters if not specified, but it is important to find the optimal number of clusters so that the data produced is well detailed and valuable.\nTo do this, we will use the KElbowVisualizer that will help find the optimal number of clusters. We will use this method and pass in a range of clusters and calculate their inertias (essentially the distances of data points to their closest cluster center). Based on the visualization, we choose the point where the curve cuts, or forms an elbow, and use that.\n\n# Perform elbow method to find optimal number of clusters\nk_means = KMeans(random_state=42)\nviz = KElbowVisualizer(k_means, k=(1, 12))\nviz.fit(X)\nviz.show()\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\nWe see that the elbow forms at k = 5, meaning 5 is the optimal number of clusters to use in this case. This matches with the inference we had when analyzing our data for the annual income scatter plot initially.\n\n\n\nNow we can go through with actually performing our KMeans. We will perform the KMeans, then take the clusters formed and append the column to our data (a copy of our data so we do not manipulate the original set), so we know which cluster each data point belongs to. Then, we plot this on a scatter plot and also include the centroids (center of each cluster) to visualize how the data was grouped.\n\n# Use 5 clusters\nk5_means = KMeans(n_clusters=5, init=\"k-means++\")\nclusters = k5_means.fit(X)\n# print(clusters)\n\n\nX_2 = X.copy() # data[['Annual Income (k$)', 'Spending Score (1-100)']]\nX_2.loc[:, 'Cluster'] = clusters.labels_ # Add Cluster column to data set\n\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score')\nplt.scatter(X_2['Annual Income (k$)'], X_2['Spending Score (1-100)'], c=X_2['Cluster'], cmap='viridis', s=50, alpha=0.5)\n\ncentroids = k5_means.cluster_centers_\nprint(centroids)\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.plot(centroids_x, centroids_y, 'ro',markersize=16, alpha = 0.5, label='centroids')\n\nplt.show()\n\n[[86.53846154 82.12820513]\n [88.2        17.11428571]\n [55.2962963  49.51851852]\n [25.72727273 79.36363636]\n [26.30434783 20.91304348]]\n\n\n\n\n\nThis clustering matches what was expected, with 5 clusters and colors that indicate which cluster each point belongs to. The red circles indicate the centroids. The centroids seem valid in their placements amongst the data. We can also see their dimensions and (x,y) positions by printing the centroids array.\nLets now go through this process again, this time with Age.\n\nX = data[['Age', 'Spending Score (1-100)']]\nX.head()\n\n\n\n\n\n\n\n\nAge\nSpending Score (1-100)\n\n\n\n\n0\n19\n39\n\n\n1\n21\n81\n\n\n2\n20\n6\n\n\n3\n23\n77\n\n\n4\n31\n40\n\n\n\n\n\n\n\nThis is our data set for this iteration of the KMeans. Let’s again try the KElbowVisualizer to find the optimal number of clusters.\n\n# Perform elbow method to find optimal number of clusters\nk_means = KMeans(random_state=42)\nviz = KElbowVisualizer(k_means, k=(2, 12))\nviz.fit(X)\nviz.show()\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\nThis time, the elbow is present at 4 clusters, indicating that 4 clusters is likely the optimal number to use for our KMeans. Let’s do the same process as before to perform our KMeans.\n\nk5_means = KMeans(n_clusters=4, init=\"k-means++\")\nclusters = k5_means.fit(X)\nprint(clusters)\n\n\nX_2 = X.copy() #data[['Age', 'Spending Score (1-100)']]\nX_2.loc[:, 'Cluster'] = clusters.labels_ # Add Cluster column to data set\n\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter(X_2['Age'], X_2['Spending Score (1-100)'], c=X_2['Cluster'], cmap='viridis', s=50, alpha=0.5)\n\ncentroids = k5_means.cluster_centers_\nprint(centroids)\n\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.plot(centroids_x, centroids_y, 'ro',markersize=16, alpha = 0.5, label='centroids')\n\nplt.show()\n\nKMeans(n_clusters=4)\n[[27.61702128 49.14893617]\n [30.1754386  82.35087719]\n [43.29166667 15.02083333]\n [55.70833333 48.22916667]]\n\n\n\n\n\nIn this case, the clusters seem to work well, but there do appear to be a significant number of outliers. The centroids appear to be accurate to where the clusters are marked. but there are points within each cluster that are fairly far from their respective cluster’s center. This indicates that clustering works well here, but outliers do seem to exist.\n\n\n\nLet’s further analyze the data we have. We do see that outliers exist in the data, and some points in each cluster seem to be significantly distant from their centers.\nTo do this, we will use Density-Based Spatial Clustering, or DBSCAN, which will group data points based on their density. It will also identify regions with high-density, and classify points with noise(meaningless or uninterpretable data) as outliers.\nTo begin the DBSCAN, we have to note 2 parameters: eps and min samples. Eps, or Epsilon, is the maximum distance we will allow to exist between two data points for them to be considered neighbors. min samples, or minimum samples, which is the number of points needed within the radius for a point to determine number of clusters.\nHowever, we first need to figure out what eps to use and how many min_samples to provide. Since we have visualized our data and know what range it could relatively be in, we can try those possible values using numpy to get our range of values.\n\nscan_X = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\neps = np.arange(7, 13, 0.5)\nmin_samples = range(1,6)\n\nWe can now perform the DBSCAN on each value and store it. We will also find the silhouette score, which is a metric that determine how well the clustering is. It will tell us how well each clustering has done given a specific eps and min samples.\n\ndb_outputs = []\n\nfor ms in min_samples:\n    for ep in eps:\n        labels = DBSCAN(eps=ep, min_samples=ms).fit(scan_X).labels_\n        sil = silhouette_score(scan_X, labels)\n        db_outputs.append((sil, ep, ms))\n\nNow we will sort the data so that we find the eps and min_samples for the entry with the highest silhouette score.\n\nsil_score, eps, min_samples = sorted(db_outputs, key=lambda x:x[-1])[-1]\nprint(\"eps: \", eps)\nprint(\"min_samples: \", min_samples)\nprint(\"Best silhouette_score: \", sil_score)\n\neps:  12.5\nmin_samples:  5\nBest silhouette_score:  0.23413283117329386\n\n\nThe optimal eps is 12.5 and min samples is 5. We can now use this to do the planned DBSCAN.\n\n# DBSCSAN\ndbscan = DBSCAN(eps=12.5, min_samples=5)\ndbscan.fit(scan_X)\ndbscan.labels_\n\narray([-1,  0, -1,  0,  0,  0, -1, -1, -1,  0, -1, -1,  1,  0, -1,  0,  0,\n        0,  0, -1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n        1,  0,  1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  2, -1,  2,  0,  2,  3,  2,  3,  2,  0,  2, -1,  2,\n        3,  2, -1,  2,  3,  2, -1,  2, -1,  2,  0,  2,  3,  2,  3,  2,  3,\n        2,  3,  2,  3,  2, -1,  2, -1,  2, -1,  2,  3,  2,  3,  2,  3,  2,\n        3,  2,  3,  2,  3,  2,  3,  2,  3,  2, -1,  2,  3,  2, -1,  2, -1,\n       -1, -1,  2, -1, -1, -1,  2, -1, -1, -1, -1, -1, -1])\n\n\nThe labels show which cluster the data point is assigned to, and gives it that label. -1 indicates it is not clustered as it did not meet the constraints provided to the DBSCAN. These can represent the outliers in our data set. Let’s take a look at how many there are.\n\narr = np.array(dbscan.labels_)\nnp.count_nonzero(arr == -1)\n\n32\n\n\nThe DBSCAN labeled 32 points as -1, so therefore 32 data points as outliers in our set. Let’s get a table look of this to see how many data points where in each cluster.\n\nX_copy = scan_X.copy()\nX_copy.loc[:, 'Cluster'] = dbscan.labels_\nX_copy.groupby('Cluster').size().to_frame()\n\n\n\n\n\n\n\n\n0\n\n\nCluster\n\n\n\n\n\n-1\n32\n\n\n0\n109\n\n\n1\n7\n\n\n2\n34\n\n\n3\n18\n\n\n\n\n\n\n\nWe see that the dbscan made 4 clusters. Cluster 0 had 109 data points, cluster 1 had 7, cluster 2 had 34, and cluster 3 had 18. The remaining 32 points were considered outliers."
  },
  {
    "objectID": "posts/Clustering_Blog_Post/Clustering.html#setup",
    "href": "posts/Clustering_Blog_Post/Clustering.html#setup",
    "title": "Clustering Blog",
    "section": "",
    "text": "We will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Numpy will also be used for array manipulation. Scikit learn (sklearn) libraries will also be imported for its KMeans Clustering and DBSCAN. We will also use a yellowbrick import for the elbow visualizer, which will help find with KMeans optimizations.\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"Mall_Customers.csv\")\nprint(data.shape)\ndata.head()\n\n(200, 5)\n\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\nThe dimensions of the data is (200, 5), so it contains the columns customerID, Gender, Age, Annual Income, and Spending Score. There are 200 data points for each individual that we can use. The customerID field will be unnecessary for our purposes, as it does not provide any valuable information. We want to know how spending score is affected by these factors on individuals, so we will start by looking at this.\n\n\n\nTo gain a better understanding of the data and what we can use, we can visualize it by plotting the features provided.\nLet’s plot Age against Spending Score and see what the scatter plot for this will look like.\n\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter(data['Age'], data['Spending Score (1-100)'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x129554850&gt;\n\n\n\n\n\nThe data appears to be spread out, and it is possible to visualize some potential clusters here. One inference is that as customers get older their spending scores fall. After age 40, the scores see a immediate drop and then plateau, as in they do not go above 60 again.\nLet’s look at Annual Income against Spending Score now.\n\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score')\nplt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1295f9b90&gt;\n\n\n\n\n\nHere we can really see clusters present. There appears to be 5 distinct clusters, roughly one at each corner and one to the center-left. This makes using the idea of using a Clustering algorithm much more grounded, and shows potential for meaningful results.\nLet’s also look at Gender really quickly and see if that happens to play a role.\n\nplt.xlabel('Gender')\nplt.ylabel('Spending Score')\nplt.scatter(data['Gender'], data['Spending Score (1-100)'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x12965b4d0&gt;\n\n\n\n\n\nA scatter plot is used to see the differences between spending scores in gender’s. From the graph, it does not appear that much difference exists, so we can dive a little deeper to truly be certain.\n\ndata['Gender'].value_counts()\n\nGender\nFemale    112\nMale       88\nName: count, dtype: int64\n\n\nThere are more Female customers in the data set than Male, which could mean something. Let’s take a look at the mean and median spending scores for each gender to see if any significant differences exists.\n\n# Find Mean and Median Spending Scores for each Gender\nfemale_data = data[data['Gender'] == 'Female']\nfemale_avg = female_data['Spending Score (1-100)'].mean()\nfemale_med = female_data['Spending Score (1-100)'].median()\nprint(\"Female Spending Score Average: \", female_avg)\nprint(\"Female Spending Score Median: \", female_med)\n\nmale_data = data[data['Gender'] == 'Male']\nmale_avg = male_data['Spending Score (1-100)'].mean()\nmale_med = male_data['Spending Score (1-100)'].median()\nprint(\"Male Spending Score Average: \", male_avg)\nprint(\"Male Spending Score Median: \", male_med)\n\nFemale Spending Score Average:  51.526785714285715\nFemale Spending Score Median:  50.0\nMale Spending Score Average:  48.51136363636363\nMale Spending Score Median:  50.0\n\n\nThe average spending score for Female’s is slightly higher, but not by very much. The median scores for both gender’s are also the same. This indicates that Gender is not too big a factor on how spending score is affected.\nThis further strengthens the argument for using a Clustering algorithm, as clustering algorithms generally do not use non-numeric data.\n\n\n\nNow that we know what to expect from our analysis and what data to look at, we can begin setting up our data to use for our model. We will be looking at Age and Annual Income and their effects on spending score. Let’s first begin with Annual Income.\n\nX = data[['Annual Income (k$)', 'Spending Score (1-100)']]\nX.head()\n\n\n\n\n\n\n\n\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n15\n39\n\n\n1\n15\n81\n\n\n2\n16\n6\n\n\n3\n16\n77\n\n\n4\n17\n40\n\n\n\n\n\n\n\nSince we are using a KMeans algorithm, we have to find the number of clusters that makes the most sense. The KMeans will default to 8 clusters if not specified, but it is important to find the optimal number of clusters so that the data produced is well detailed and valuable.\nTo do this, we will use the KElbowVisualizer that will help find the optimal number of clusters. We will use this method and pass in a range of clusters and calculate their inertias (essentially the distances of data points to their closest cluster center). Based on the visualization, we choose the point where the curve cuts, or forms an elbow, and use that.\n\n# Perform elbow method to find optimal number of clusters\nk_means = KMeans(random_state=42)\nviz = KElbowVisualizer(k_means, k=(1, 12))\nviz.fit(X)\nviz.show()\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\nWe see that the elbow forms at k = 5, meaning 5 is the optimal number of clusters to use in this case. This matches with the inference we had when analyzing our data for the annual income scatter plot initially.\n\n\n\nNow we can go through with actually performing our KMeans. We will perform the KMeans, then take the clusters formed and append the column to our data (a copy of our data so we do not manipulate the original set), so we know which cluster each data point belongs to. Then, we plot this on a scatter plot and also include the centroids (center of each cluster) to visualize how the data was grouped.\n\n# Use 5 clusters\nk5_means = KMeans(n_clusters=5, init=\"k-means++\")\nclusters = k5_means.fit(X)\n# print(clusters)\n\n\nX_2 = X.copy() # data[['Annual Income (k$)', 'Spending Score (1-100)']]\nX_2.loc[:, 'Cluster'] = clusters.labels_ # Add Cluster column to data set\n\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score')\nplt.scatter(X_2['Annual Income (k$)'], X_2['Spending Score (1-100)'], c=X_2['Cluster'], cmap='viridis', s=50, alpha=0.5)\n\ncentroids = k5_means.cluster_centers_\nprint(centroids)\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.plot(centroids_x, centroids_y, 'ro',markersize=16, alpha = 0.5, label='centroids')\n\nplt.show()\n\n[[86.53846154 82.12820513]\n [88.2        17.11428571]\n [55.2962963  49.51851852]\n [25.72727273 79.36363636]\n [26.30434783 20.91304348]]\n\n\n\n\n\nThis clustering matches what was expected, with 5 clusters and colors that indicate which cluster each point belongs to. The red circles indicate the centroids. The centroids seem valid in their placements amongst the data. We can also see their dimensions and (x,y) positions by printing the centroids array.\nLets now go through this process again, this time with Age.\n\nX = data[['Age', 'Spending Score (1-100)']]\nX.head()\n\n\n\n\n\n\n\n\nAge\nSpending Score (1-100)\n\n\n\n\n0\n19\n39\n\n\n1\n21\n81\n\n\n2\n20\n6\n\n\n3\n23\n77\n\n\n4\n31\n40\n\n\n\n\n\n\n\nThis is our data set for this iteration of the KMeans. Let’s again try the KElbowVisualizer to find the optimal number of clusters.\n\n# Perform elbow method to find optimal number of clusters\nk_means = KMeans(random_state=42)\nviz = KElbowVisualizer(k_means, k=(2, 12))\nviz.fit(X)\nviz.show()\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\nThis time, the elbow is present at 4 clusters, indicating that 4 clusters is likely the optimal number to use for our KMeans. Let’s do the same process as before to perform our KMeans.\n\nk5_means = KMeans(n_clusters=4, init=\"k-means++\")\nclusters = k5_means.fit(X)\nprint(clusters)\n\n\nX_2 = X.copy() #data[['Age', 'Spending Score (1-100)']]\nX_2.loc[:, 'Cluster'] = clusters.labels_ # Add Cluster column to data set\n\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter(X_2['Age'], X_2['Spending Score (1-100)'], c=X_2['Cluster'], cmap='viridis', s=50, alpha=0.5)\n\ncentroids = k5_means.cluster_centers_\nprint(centroids)\n\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.plot(centroids_x, centroids_y, 'ro',markersize=16, alpha = 0.5, label='centroids')\n\nplt.show()\n\nKMeans(n_clusters=4)\n[[27.61702128 49.14893617]\n [30.1754386  82.35087719]\n [43.29166667 15.02083333]\n [55.70833333 48.22916667]]\n\n\n\n\n\nIn this case, the clusters seem to work well, but there do appear to be a significant number of outliers. The centroids appear to be accurate to where the clusters are marked. but there are points within each cluster that are fairly far from their respective cluster’s center. This indicates that clustering works well here, but outliers do seem to exist.\n\n\n\nLet’s further analyze the data we have. We do see that outliers exist in the data, and some points in each cluster seem to be significantly distant from their centers.\nTo do this, we will use Density-Based Spatial Clustering, or DBSCAN, which will group data points based on their density. It will also identify regions with high-density, and classify points with noise(meaningless or uninterpretable data) as outliers.\nTo begin the DBSCAN, we have to note 2 parameters: eps and min samples. Eps, or Epsilon, is the maximum distance we will allow to exist between two data points for them to be considered neighbors. min samples, or minimum samples, which is the number of points needed within the radius for a point to determine number of clusters.\nHowever, we first need to figure out what eps to use and how many min_samples to provide. Since we have visualized our data and know what range it could relatively be in, we can try those possible values using numpy to get our range of values.\n\nscan_X = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\neps = np.arange(7, 13, 0.5)\nmin_samples = range(1,6)\n\nWe can now perform the DBSCAN on each value and store it. We will also find the silhouette score, which is a metric that determine how well the clustering is. It will tell us how well each clustering has done given a specific eps and min samples.\n\ndb_outputs = []\n\nfor ms in min_samples:\n    for ep in eps:\n        labels = DBSCAN(eps=ep, min_samples=ms).fit(scan_X).labels_\n        sil = silhouette_score(scan_X, labels)\n        db_outputs.append((sil, ep, ms))\n\nNow we will sort the data so that we find the eps and min_samples for the entry with the highest silhouette score.\n\nsil_score, eps, min_samples = sorted(db_outputs, key=lambda x:x[-1])[-1]\nprint(\"eps: \", eps)\nprint(\"min_samples: \", min_samples)\nprint(\"Best silhouette_score: \", sil_score)\n\neps:  12.5\nmin_samples:  5\nBest silhouette_score:  0.23413283117329386\n\n\nThe optimal eps is 12.5 and min samples is 5. We can now use this to do the planned DBSCAN.\n\n# DBSCSAN\ndbscan = DBSCAN(eps=12.5, min_samples=5)\ndbscan.fit(scan_X)\ndbscan.labels_\n\narray([-1,  0, -1,  0,  0,  0, -1, -1, -1,  0, -1, -1,  1,  0, -1,  0,  0,\n        0,  0, -1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n        1,  0,  1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  2, -1,  2,  0,  2,  3,  2,  3,  2,  0,  2, -1,  2,\n        3,  2, -1,  2,  3,  2, -1,  2, -1,  2,  0,  2,  3,  2,  3,  2,  3,\n        2,  3,  2,  3,  2, -1,  2, -1,  2, -1,  2,  3,  2,  3,  2,  3,  2,\n        3,  2,  3,  2,  3,  2,  3,  2,  3,  2, -1,  2,  3,  2, -1,  2, -1,\n       -1, -1,  2, -1, -1, -1,  2, -1, -1, -1, -1, -1, -1])\n\n\nThe labels show which cluster the data point is assigned to, and gives it that label. -1 indicates it is not clustered as it did not meet the constraints provided to the DBSCAN. These can represent the outliers in our data set. Let’s take a look at how many there are.\n\narr = np.array(dbscan.labels_)\nnp.count_nonzero(arr == -1)\n\n32\n\n\nThe DBSCAN labeled 32 points as -1, so therefore 32 data points as outliers in our set. Let’s get a table look of this to see how many data points where in each cluster.\n\nX_copy = scan_X.copy()\nX_copy.loc[:, 'Cluster'] = dbscan.labels_\nX_copy.groupby('Cluster').size().to_frame()\n\n\n\n\n\n\n\n\n0\n\n\nCluster\n\n\n\n\n\n-1\n32\n\n\n0\n109\n\n\n1\n7\n\n\n2\n34\n\n\n3\n18\n\n\n\n\n\n\n\nWe see that the dbscan made 4 clusters. Cluster 0 had 109 data points, cluster 1 had 7, cluster 2 had 34, and cluster 3 had 18. The remaining 32 points were considered outliers."
  },
  {
    "objectID": "posts/Anomaly_Detection_Blog_Post/Anomaly_Detection.html",
    "href": "posts/Anomaly_Detection_Blog_Post/Anomaly_Detection.html",
    "title": "Outlier/Anomaly Detection Blog",
    "section": "",
    "text": "This blog post will explore how to find and detect anomalies in data sets. The data set that is used for this is a Walmart data set that provides store and sales information. The goal will be to detect outliers and whether they exist, and how to handle them.\n\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Numpy will also be used for some array manipulation. Scikit learn (sklearn) libraries will also be imported for its metrics and models. We will use the metrics to see how accurate the model is and find the error it produces. The model libraries will be used to build the Linear Regression and also split the data.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import mean_squared_error\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv('walmart-data.csv')\nprint(data.shape)\ndata.head()\n\n(421570, 17)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nStore\nDate\nIsHoliday\nDept\nWeekly_Sales\nTemperature\nFuel_Price\nMarkDown1\nMarkDown2\nMarkDown3\nMarkDown4\nMarkDown5\nCPI\nUnemployment\nType\nSize\n\n\n\n\n0\n0\n1\n2010-02-05\n0\n1.0\n24924.50\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n1\n1\n1\n2010-02-05\n0\n26.0\n11737.12\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n2\n2\n1\n2010-02-05\n0\n17.0\n13223.76\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n3\n3\n1\n2010-02-05\n0\n45.0\n37.44\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n4\n4\n1\n2010-02-05\n0\n28.0\n1085.29\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n\n\n\n\n\nThe dimensions of the data is (421570, 17). There were 16 main categories provided that gave information on Walmart. There are 421,570 data entries, so there is a lot of data that can be utilized. We want to see what really drives weekly sales, so that will be what to compare against for each other category.\n\n\n\nLet’s plot the data to visualize what the distributions look like and see if we can draw any initial inferences from what we see. By looking at the plotted data, sometimes it can be fairly clear if outliers do exist.\nFirst, let’s look at how CPI (consumer price index) at the time affects the weekly sales.\n\nplt.xlabel('CPI')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['CPI'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x12384c910&gt;\n\n\n\n\n\nThere do appear to be some outliers we can see immediately. The data point at the top left of the plot do not fit with the other clusters of data present. Generally all the points in to 0 to 140 range remain below 400,000, with the excpetion of the few way above.\nNow let’s look at the temperature on the days of sales and whether that plays a role in how weekly sales are affected.\n\nplt.xlabel('Temperature')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['Temperature'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1238da250&gt;\n\n\n\n\n\nGenerally, the temperature does not make too big of a difference, and normal days do have better sales than those with temperatures that are too cold or too hot. The 40 to 60 degrees range is ideal, but in this range we do see some outliers that are significantly higher up than all other data points.\nLet’s take a look at how the unemployment rate at the time affected the weekly sales.\n\nplt.xlabel('Unemployment Rate')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['Unemployment'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1238e9ed0&gt;\n\n\n\n\n\nThe range of between 7 to 10 percent unemployment where sales were the highest. Again however, in the 8 to 10 percent range there do appear to be a few outliers that exist much higher than all other data points.\nNow let’s see how the holidays drive weekly sales. A holiday is represented by 1, and a non-holiday is represented by 0.\n\nplt.xlabel('Holiday')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['IsHoliday'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1238cea90&gt;\n\n\n\n\n\nHolidays do seem to bring in more sales on average. For non-holidays, there do not really appear to be outliers, but for holidays there seems to be a few towards to upper end, but is it difficult to tell.\n\n\n\nNow that we have a better understanding of what the data looks like and what we are searching for, we can begin our analysis on the data set and find outliers.\nLet’s create the data set and drop the categories we do not need to take into account for this.\n\nX = data[['IsHoliday', 'Dept', 'Weekly_Sales',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size']]\nX.head()\n\n\n\n\n\n\n\n\nIsHoliday\nDept\nWeekly_Sales\nTemperature\nFuel_Price\nMarkDown1\nMarkDown2\nMarkDown3\nMarkDown4\nMarkDown5\nCPI\nUnemployment\nType\nSize\n\n\n\n\n0\n0\n1.0\n24924.50\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n1\n0\n26.0\n11737.12\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n2\n0\n17.0\n13223.76\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n3\n0\n45.0\n37.44\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n4\n0\n28.0\n1085.29\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n\n\n\n\n\n\n\n\nFor detecting anomalies/outliers, we will use Density-Based Spatial Clustering, or DBSCAN, which will group data points based on their density. It will identify regions with high-density, and classify points with noise(meaningless/irrelevant data) as outliers.\nTo begin the DBSCAN, we have to note 2 parameters: eps and min samples. Eps, or Epsilon, is the maximum distance we will allow to exist between two data points for them to be considered neighbors. min samples, or minimum samples, which is the number of points needed within the radius for a point to determine number of clusters.\nBecause we know our data is widespread and can hold a big range, we will set our eps to 1000 and provide it a min_samples of 4.\n\n# DBSCAN\ndbscan = DBSCAN(eps = 1000, min_samples= 4)\ndbscan.fit(X)\ndbscan.labels_\n\narray([   0,    0,    0, ..., 5893, 5893, 5893])\n\n\nLet’s see how many outliers the DBSCAN detected.\n\n# Outlier count\nlabels = dbscan.labels_\narr = np.array(labels)\nprint(\"Number of Outliers: \", np.count_nonzero(arr == -1))\nprint(\"Percentage of Outliers: \", (sum(labels == -1) / X.shape[0]) * 100, \"%\")\n\nNumber of Outliers:  38479\nPercentage of Outliers:  9.127547026591076 %\n\n\nAbout 9 percent of the data in this data set is being classified as anomalies/outliers.\nLet’s visualize how the data was clustered through a table.\n\nX_copy = X.copy()\nX_copy.loc[:, 'Cluster'] = dbscan.labels_\nX_copy.groupby('Cluster').size().to_frame()\n\n\n\n\n\n\n\n\n0\n\n\nCluster\n\n\n\n\n\n-1\n38479\n\n\n0\n6381\n\n\n1\n117\n\n\n2\n23\n\n\n3\n5\n\n\n...\n...\n\n\n5892\n7\n\n\n5893\n36\n\n\n5894\n6\n\n\n5895\n4\n\n\n5896\n5\n\n\n\n\n5898 rows × 1 columns\n\n\n\nThis DBSCAN created almost 6000 clusters, and marked 38,479 as outliers. We can conclude that this data set does have several anomalies/outliers."
  },
  {
    "objectID": "posts/Anomaly_Detection_Blog_Post/Anomaly_Detection.html#setup",
    "href": "posts/Anomaly_Detection_Blog_Post/Anomaly_Detection.html#setup",
    "title": "Outlier/Anomaly Detection Blog",
    "section": "",
    "text": "We will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Numpy will also be used for some array manipulation. Scikit learn (sklearn) libraries will also be imported for its metrics and models. We will use the metrics to see how accurate the model is and find the error it produces. The model libraries will be used to build the Linear Regression and also split the data.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import mean_squared_error\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv('walmart-data.csv')\nprint(data.shape)\ndata.head()\n\n(421570, 17)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nStore\nDate\nIsHoliday\nDept\nWeekly_Sales\nTemperature\nFuel_Price\nMarkDown1\nMarkDown2\nMarkDown3\nMarkDown4\nMarkDown5\nCPI\nUnemployment\nType\nSize\n\n\n\n\n0\n0\n1\n2010-02-05\n0\n1.0\n24924.50\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n1\n1\n1\n2010-02-05\n0\n26.0\n11737.12\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n2\n2\n1\n2010-02-05\n0\n17.0\n13223.76\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n3\n3\n1\n2010-02-05\n0\n45.0\n37.44\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n4\n4\n1\n2010-02-05\n0\n28.0\n1085.29\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n\n\n\n\n\nThe dimensions of the data is (421570, 17). There were 16 main categories provided that gave information on Walmart. There are 421,570 data entries, so there is a lot of data that can be utilized. We want to see what really drives weekly sales, so that will be what to compare against for each other category.\n\n\n\nLet’s plot the data to visualize what the distributions look like and see if we can draw any initial inferences from what we see. By looking at the plotted data, sometimes it can be fairly clear if outliers do exist.\nFirst, let’s look at how CPI (consumer price index) at the time affects the weekly sales.\n\nplt.xlabel('CPI')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['CPI'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x12384c910&gt;\n\n\n\n\n\nThere do appear to be some outliers we can see immediately. The data point at the top left of the plot do not fit with the other clusters of data present. Generally all the points in to 0 to 140 range remain below 400,000, with the excpetion of the few way above.\nNow let’s look at the temperature on the days of sales and whether that plays a role in how weekly sales are affected.\n\nplt.xlabel('Temperature')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['Temperature'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1238da250&gt;\n\n\n\n\n\nGenerally, the temperature does not make too big of a difference, and normal days do have better sales than those with temperatures that are too cold or too hot. The 40 to 60 degrees range is ideal, but in this range we do see some outliers that are significantly higher up than all other data points.\nLet’s take a look at how the unemployment rate at the time affected the weekly sales.\n\nplt.xlabel('Unemployment Rate')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['Unemployment'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1238e9ed0&gt;\n\n\n\n\n\nThe range of between 7 to 10 percent unemployment where sales were the highest. Again however, in the 8 to 10 percent range there do appear to be a few outliers that exist much higher than all other data points.\nNow let’s see how the holidays drive weekly sales. A holiday is represented by 1, and a non-holiday is represented by 0.\n\nplt.xlabel('Holiday')\nplt.ylabel('Weekly Sales')\nplt.scatter(data['IsHoliday'], data['Weekly_Sales'], color='g')\n\n&lt;matplotlib.collections.PathCollection at 0x1238cea90&gt;\n\n\n\n\n\nHolidays do seem to bring in more sales on average. For non-holidays, there do not really appear to be outliers, but for holidays there seems to be a few towards to upper end, but is it difficult to tell.\n\n\n\nNow that we have a better understanding of what the data looks like and what we are searching for, we can begin our analysis on the data set and find outliers.\nLet’s create the data set and drop the categories we do not need to take into account for this.\n\nX = data[['IsHoliday', 'Dept', 'Weekly_Sales',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size']]\nX.head()\n\n\n\n\n\n\n\n\nIsHoliday\nDept\nWeekly_Sales\nTemperature\nFuel_Price\nMarkDown1\nMarkDown2\nMarkDown3\nMarkDown4\nMarkDown5\nCPI\nUnemployment\nType\nSize\n\n\n\n\n0\n0\n1.0\n24924.50\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n1\n0\n26.0\n11737.12\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n2\n0\n17.0\n13223.76\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n3\n0\n45.0\n37.44\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n4\n0\n28.0\n1085.29\n42.31\n2.572\n0.0\n0.0\n0.0\n0.0\n0.0\n211.096358\n8.106\n3\n151315\n\n\n\n\n\n\n\n\n\n\nFor detecting anomalies/outliers, we will use Density-Based Spatial Clustering, or DBSCAN, which will group data points based on their density. It will identify regions with high-density, and classify points with noise(meaningless/irrelevant data) as outliers.\nTo begin the DBSCAN, we have to note 2 parameters: eps and min samples. Eps, or Epsilon, is the maximum distance we will allow to exist between two data points for them to be considered neighbors. min samples, or minimum samples, which is the number of points needed within the radius for a point to determine number of clusters.\nBecause we know our data is widespread and can hold a big range, we will set our eps to 1000 and provide it a min_samples of 4.\n\n# DBSCAN\ndbscan = DBSCAN(eps = 1000, min_samples= 4)\ndbscan.fit(X)\ndbscan.labels_\n\narray([   0,    0,    0, ..., 5893, 5893, 5893])\n\n\nLet’s see how many outliers the DBSCAN detected.\n\n# Outlier count\nlabels = dbscan.labels_\narr = np.array(labels)\nprint(\"Number of Outliers: \", np.count_nonzero(arr == -1))\nprint(\"Percentage of Outliers: \", (sum(labels == -1) / X.shape[0]) * 100, \"%\")\n\nNumber of Outliers:  38479\nPercentage of Outliers:  9.127547026591076 %\n\n\nAbout 9 percent of the data in this data set is being classified as anomalies/outliers.\nLet’s visualize how the data was clustered through a table.\n\nX_copy = X.copy()\nX_copy.loc[:, 'Cluster'] = dbscan.labels_\nX_copy.groupby('Cluster').size().to_frame()\n\n\n\n\n\n\n\n\n0\n\n\nCluster\n\n\n\n\n\n-1\n38479\n\n\n0\n6381\n\n\n1\n117\n\n\n2\n23\n\n\n3\n5\n\n\n...\n...\n\n\n5892\n7\n\n\n5893\n36\n\n\n5894\n6\n\n\n5895\n4\n\n\n5896\n5\n\n\n\n\n5898 rows × 1 columns\n\n\n\nThis DBSCAN created almost 6000 clusters, and marked 38,479 as outliers. We can conclude that this data set does have several anomalies/outliers."
  },
  {
    "objectID": "posts/Classification_Blog_Post/Classification.html",
    "href": "posts/Classification_Blog_Post/Classification.html",
    "title": "Classification Blog",
    "section": "",
    "text": "This blog post will walk through data analysis with Classification. The data set contains several data points on individuals and which drug they were administered. These data points include information of each individual’s age, sex, blood pressure, cholesterol, sodium to potassium ratio, and the drug they were administered. The goal here is to determine what factors determine which drug to administer to the individual, as well as find which factors play a big role in the drug classification. This classification will be used to predict which drug should be administered to future patients based on the factors provided.\n\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Scikit learn (sklearn) libraries will also be imported for its metrics, model selection, finding importance of features, and the classifier.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_curve, precision_score, recall_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"drug.csv\")\nprint(data.shape)\ndata.head()\n\n(200, 6)\n\n\n\n\n\n\n\n\n\nAge\nSex\nBP\nCholesterol\nNa_to_K\nDrug\n\n\n\n\n0\n23\nF\nHIGH\nHIGH\n25.355\nDrugY\n\n\n1\n47\nM\nLOW\nHIGH\n13.093\ndrugC\n\n\n2\n47\nM\nLOW\nHIGH\n10.114\ndrugC\n\n\n3\n28\nF\nNORMAL\nHIGH\n7.798\ndrugX\n\n\n4\n61\nF\nLOW\nHIGH\n18.043\nDrugY\n\n\n\n\n\n\n\nThe dimensions of the data is (200, 6), which means that the data has 6 columns per each individual: Age, Sex, Blood Pressure (BP), Cholesterol, Sodium to Potassium Ratio (Na_to_K), and Drug. There are 200 entries in the data set, which is generally considered a low amount of data to have, but for our purposes we will make do with this. We will see how these factors of each individual play a role in what drug they are administered.\n\n\n\nTo gain a better understanding of the data, we can visualize it by plotting some of the factors.\n\n\n\nLet’s first look at the different types of drugs that were administered and their frequencies in the data set.\n\nplt.hist(data['Drug'], rwidth=0.5)\n\n(array([91.,  0., 16.,  0.,  0., 54.,  0., 23.,  0., 16.]),\n array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\nIt appears that Drug Y was the most frequently administered drug, with Drug X coming in second. In total, there were 5 types of drugs that were administered, but the remaining three were administered significantly less than Drug Y and Drug X.\n\nplt.hist(data['Age'], rwidth=0.5)\n\n(array([16., 22., 20., 20., 21., 28., 16., 23., 18., 16.]),\n array([15. , 20.9, 26.8, 32.7, 38.6, 44.5, 50.4, 56.3, 62.2, 68.1, 74. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\nThe age groups seem to roughly evenly distributed. There appears to be an equal amount of individuals below 50 and above 50, and no significant outlying age group.\nLet’s try plotting Age against the Drug type to see what information that may provide.\n\nplt.xlabel('Drug Type')\nplt.ylabel('Age')\nplt.scatter(data['Drug'], data['Age'])\nplt.show()\n\n\n\n\nThis somewhat matches our expectations, as we see more data points for Drug Y and Drug X since those were the most frequently administered drugs. Drug C has very few points and is roughly evenly spread out among all age groups. Drug A seems to be only administered for individuals below 50, while Drug B seems to be only administered for individuals above 50.\nLet’s try a similar plot, but this time plotting the Sodium to Potassium ratio against the Drug Type.\n\nplt.xlabel('Drug Type')\nplt.ylabel('Sodium to Potassium Ratio')\nplt.scatter(data['Drug'], data['Na_to_K'])\nplt.show()\n\n\n\n\nThose with a ratio of over 15 were always administered Drug Y. If the ratio was below 15, then any one of the other four drug types were given. This indicates that the Sodium to Potassium ratio plays a huge role in classifying which drug should be applied.\n\n\n\nNow that we have looked at a few data points and gotten a better understanding of what we are looking for and what to expect, we can begin preparing the data to be placed into our Classification model. The data will of course need to be split into training and testing sets, for which the standard 80-20 convention will be followed.\n\n# Prepare data and do split data into training and test groups (Using standard 80-20 split)\nX = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']]\nX = pd.get_dummies(X) # Convert categorical vars to indicator vars\nY = data['Drug']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, shuffle=True, random_state=42)\nprint(\"X-Training Set Dimensions: \", X_train.shape)\nprint(\"X-Test Set Dimensions: \", X_test.shape)\nprint(\"Y-Training Set Dimensions: \", Y_train.shape)\nprint(\"Y-Test Set Dimensions: \", Y_test.shape)\n\nX-Training Set Dimensions:  (160, 9)\nX-Test Set Dimensions:  (40, 9)\nY-Training Set Dimensions:  (160,)\nY-Test Set Dimensions:  (40,)\n\n\nWe use the Pandas get_dummies() method to convert any categorical variables that we have into 1/0 indicator values. For example, our data would indicate Cholesterol as either HIGH, NORMAL, or LOW. This pandas method will split the data into three new categories called BP_HIGH, BP_NORMAL, and BP_Low. In these categories, a 1 will indicate that this is true for the indivdual, and a 0 will indicate that it is not true. This is applied for all parts of the data set.\n\n# Display what Pandas get_dummies has done to the data set.\nX.head()\n\n\n\n\n\n\n\n\nAge\nNa_to_K\nSex_F\nSex_M\nBP_HIGH\nBP_LOW\nBP_NORMAL\nCholesterol_HIGH\nCholesterol_NORMAL\n\n\n\n\n0\n23\n25.355\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n47\n13.093\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n47\n10.114\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n28\n7.798\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n4\n61\n18.043\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n\nWe can now go through with the Classification, for which we will use the Random Forest Classification Algorithm. We will do a cross validation to see the expected level of fit for our model to the data set.\n\n# Use Random Forest for Classification\nforest = RandomForestClassifier(n_estimators=200, random_state=42)\ncross = cross_val_predict(forest, X_train, Y_train, cv=None, method=\"predict_proba\")\ncross[:5]\n\narray([[0.01 , 0.03 , 0.   , 0.03 , 0.93 ],\n       [0.   , 0.   , 0.005, 0.   , 0.995],\n       [0.01 , 0.01 , 0.   , 0.   , 0.98 ],\n       [0.96 , 0.   , 0.   , 0.02 , 0.02 ],\n       [0.99 , 0.   , 0.   , 0.   , 0.01 ]])\n\n\nWe can also find the cross validation score to understand the model’s performance better and how accurate it is. The cross_val_score method will be used, and we will use the default cross validation (cv) of 5, so this model is trained/tested on 5 differents subsets of the data.\n\ncross_val_score(forest, X_train, Y_train, scoring='accuracy')\n\narray([1.     , 1.     , 1.     , 0.96875, 1.     ])\n\n\nThe model seems to work really well, as our accuracy scores are very high for each section.\n\n\n\nWe can now go ahead and fit our model to the Random Forest model we plan to use.\n\nforest.fit(X_train, Y_train)\npredict = forest.predict(X_test)\n\nWe will now apply the Precision/Recall method using sklearn’s metrics precision and recall to determine how well our Classification model did. The precision will compare the number of true positive and false positives, and the recall will compare true positives and false negatives. From sklearn’s metrics we will also find the accuracy score to see how accurate our model is.\n\n# precision = tp / (tp + fp)\nprint(\"Precision Scores: \", precision_score(Y_test, predict, average=None))\n\n# recall = tp / (tp + fn)\nprint(\"Recall Scores: \", recall_score(Y_test, predict, average=None))\n\nprint(\"Random Forest Model Accuracy: \", accuracy_score(Y_test, predict))\n\nPrecision Scores:  [1. 1. 1. 1. 1.]\nRecall Scores:  [1. 1. 1. 1. 1.]\nRandom Forest Model Accuracy:  1.0\n\n\nFor every performance metric our model shows a 1.0, indicating it is 100% accurate to compared to the data set. This means that our model produces no false positives or false negatives. The Random Forest classifier fits the data very well.\nWe can also look at it side-by-side through a table, and see how the model predicted performance comapres to the actual performance.\n\n# Side by Side comparison\npd.DataFrame({\"Actual Performance: \" : Y_test[:10], \"Model Predicted Performance\" : predict[:10]})\n\n\n\n\n\n\n\n\nActual Performance:\nModel Predicted Performance\n\n\n\n\n95\ndrugX\ndrugX\n\n\n15\nDrugY\nDrugY\n\n\n30\ndrugX\ndrugX\n\n\n158\ndrugC\ndrugC\n\n\n128\nDrugY\nDrugY\n\n\n115\nDrugY\nDrugY\n\n\n69\nDrugY\nDrugY\n\n\n170\ndrugX\ndrugX\n\n\n174\ndrugA\ndrugA\n\n\n45\ndrugX\ndrugX\n\n\n\n\n\n\n\n\n\n\nNow that we see how well our model works, let’s look at which factors, or features, our Random Forest actually used to make decisions. As we noticed at the beginning, there were some features that appeared to heavily influence which drug was chosen.\nWe will use sklearn’s feature selection capabilities for this through the SelectFromModel option. This will give us the ability to look at what features were used.\n\n# Let's look at which features were the most important. Using select from model\nmodel = SelectFromModel(RandomForestClassifier(n_estimators=200, random_state=42))\nmodel.fit(X_train, Y_train)\nmodel.get_support()[:5]\n\narray([ True,  True, False, False,  True])\n\n\nThis tells us that 3 of the 5 features given were used by the model to perform the classification. This is useful, but we also want to know exactly which 3 features were used, and will have to display that.\n\n# See which features were used by Random Forest\nfeatures = X_train.columns[(model.get_support())]\nprint(features)\n\nIndex(['Age', 'Na_to_K', 'BP_HIGH'], dtype='object')\n\n\nThe 3 features used were Age, Sodium to Potassium ratio, and BP_HIGH. These 3 features had the biggest impact in decision making, so they were used by the classifier. As we analyzed above from the plots, we saw how Age and the Sodium to Potassium ratio did seem to affect which drug was administered, so this was expected.\nLet’s plot the distribution of our features importance, to see how important each was. We will set our nlargest to 3, so that our 3 main features will be displayed.\n\npd.Series(model.estimator_.feature_importances_, index=X.columns).nlargest(3).plot(kind='barh')\n\n&lt;Axes: &gt;\n\n\n\n\n\nIt appears that the Sodium to Potassium ratio was by far the most important factor in the model’s classification, when it comes to deciding which drug to administer."
  },
  {
    "objectID": "posts/Classification_Blog_Post/Classification.html#setup",
    "href": "posts/Classification_Blog_Post/Classification.html#setup",
    "title": "Classification Blog",
    "section": "",
    "text": "We will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Scikit learn (sklearn) libraries will also be imported for its metrics, model selection, finding importance of features, and the classifier.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_curve, precision_score, recall_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"drug.csv\")\nprint(data.shape)\ndata.head()\n\n(200, 6)\n\n\n\n\n\n\n\n\n\nAge\nSex\nBP\nCholesterol\nNa_to_K\nDrug\n\n\n\n\n0\n23\nF\nHIGH\nHIGH\n25.355\nDrugY\n\n\n1\n47\nM\nLOW\nHIGH\n13.093\ndrugC\n\n\n2\n47\nM\nLOW\nHIGH\n10.114\ndrugC\n\n\n3\n28\nF\nNORMAL\nHIGH\n7.798\ndrugX\n\n\n4\n61\nF\nLOW\nHIGH\n18.043\nDrugY\n\n\n\n\n\n\n\nThe dimensions of the data is (200, 6), which means that the data has 6 columns per each individual: Age, Sex, Blood Pressure (BP), Cholesterol, Sodium to Potassium Ratio (Na_to_K), and Drug. There are 200 entries in the data set, which is generally considered a low amount of data to have, but for our purposes we will make do with this. We will see how these factors of each individual play a role in what drug they are administered.\n\n\n\nTo gain a better understanding of the data, we can visualize it by plotting some of the factors.\n\n\n\nLet’s first look at the different types of drugs that were administered and their frequencies in the data set.\n\nplt.hist(data['Drug'], rwidth=0.5)\n\n(array([91.,  0., 16.,  0.,  0., 54.,  0., 23.,  0., 16.]),\n array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\nIt appears that Drug Y was the most frequently administered drug, with Drug X coming in second. In total, there were 5 types of drugs that were administered, but the remaining three were administered significantly less than Drug Y and Drug X.\n\nplt.hist(data['Age'], rwidth=0.5)\n\n(array([16., 22., 20., 20., 21., 28., 16., 23., 18., 16.]),\n array([15. , 20.9, 26.8, 32.7, 38.6, 44.5, 50.4, 56.3, 62.2, 68.1, 74. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\nThe age groups seem to roughly evenly distributed. There appears to be an equal amount of individuals below 50 and above 50, and no significant outlying age group.\nLet’s try plotting Age against the Drug type to see what information that may provide.\n\nplt.xlabel('Drug Type')\nplt.ylabel('Age')\nplt.scatter(data['Drug'], data['Age'])\nplt.show()\n\n\n\n\nThis somewhat matches our expectations, as we see more data points for Drug Y and Drug X since those were the most frequently administered drugs. Drug C has very few points and is roughly evenly spread out among all age groups. Drug A seems to be only administered for individuals below 50, while Drug B seems to be only administered for individuals above 50.\nLet’s try a similar plot, but this time plotting the Sodium to Potassium ratio against the Drug Type.\n\nplt.xlabel('Drug Type')\nplt.ylabel('Sodium to Potassium Ratio')\nplt.scatter(data['Drug'], data['Na_to_K'])\nplt.show()\n\n\n\n\nThose with a ratio of over 15 were always administered Drug Y. If the ratio was below 15, then any one of the other four drug types were given. This indicates that the Sodium to Potassium ratio plays a huge role in classifying which drug should be applied.\n\n\n\nNow that we have looked at a few data points and gotten a better understanding of what we are looking for and what to expect, we can begin preparing the data to be placed into our Classification model. The data will of course need to be split into training and testing sets, for which the standard 80-20 convention will be followed.\n\n# Prepare data and do split data into training and test groups (Using standard 80-20 split)\nX = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']]\nX = pd.get_dummies(X) # Convert categorical vars to indicator vars\nY = data['Drug']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, shuffle=True, random_state=42)\nprint(\"X-Training Set Dimensions: \", X_train.shape)\nprint(\"X-Test Set Dimensions: \", X_test.shape)\nprint(\"Y-Training Set Dimensions: \", Y_train.shape)\nprint(\"Y-Test Set Dimensions: \", Y_test.shape)\n\nX-Training Set Dimensions:  (160, 9)\nX-Test Set Dimensions:  (40, 9)\nY-Training Set Dimensions:  (160,)\nY-Test Set Dimensions:  (40,)\n\n\nWe use the Pandas get_dummies() method to convert any categorical variables that we have into 1/0 indicator values. For example, our data would indicate Cholesterol as either HIGH, NORMAL, or LOW. This pandas method will split the data into three new categories called BP_HIGH, BP_NORMAL, and BP_Low. In these categories, a 1 will indicate that this is true for the indivdual, and a 0 will indicate that it is not true. This is applied for all parts of the data set.\n\n# Display what Pandas get_dummies has done to the data set.\nX.head()\n\n\n\n\n\n\n\n\nAge\nNa_to_K\nSex_F\nSex_M\nBP_HIGH\nBP_LOW\nBP_NORMAL\nCholesterol_HIGH\nCholesterol_NORMAL\n\n\n\n\n0\n23\n25.355\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n47\n13.093\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n47\n10.114\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n28\n7.798\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n4\n61\n18.043\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n\nWe can now go through with the Classification, for which we will use the Random Forest Classification Algorithm. We will do a cross validation to see the expected level of fit for our model to the data set.\n\n# Use Random Forest for Classification\nforest = RandomForestClassifier(n_estimators=200, random_state=42)\ncross = cross_val_predict(forest, X_train, Y_train, cv=None, method=\"predict_proba\")\ncross[:5]\n\narray([[0.01 , 0.03 , 0.   , 0.03 , 0.93 ],\n       [0.   , 0.   , 0.005, 0.   , 0.995],\n       [0.01 , 0.01 , 0.   , 0.   , 0.98 ],\n       [0.96 , 0.   , 0.   , 0.02 , 0.02 ],\n       [0.99 , 0.   , 0.   , 0.   , 0.01 ]])\n\n\nWe can also find the cross validation score to understand the model’s performance better and how accurate it is. The cross_val_score method will be used, and we will use the default cross validation (cv) of 5, so this model is trained/tested on 5 differents subsets of the data.\n\ncross_val_score(forest, X_train, Y_train, scoring='accuracy')\n\narray([1.     , 1.     , 1.     , 0.96875, 1.     ])\n\n\nThe model seems to work really well, as our accuracy scores are very high for each section.\n\n\n\nWe can now go ahead and fit our model to the Random Forest model we plan to use.\n\nforest.fit(X_train, Y_train)\npredict = forest.predict(X_test)\n\nWe will now apply the Precision/Recall method using sklearn’s metrics precision and recall to determine how well our Classification model did. The precision will compare the number of true positive and false positives, and the recall will compare true positives and false negatives. From sklearn’s metrics we will also find the accuracy score to see how accurate our model is.\n\n# precision = tp / (tp + fp)\nprint(\"Precision Scores: \", precision_score(Y_test, predict, average=None))\n\n# recall = tp / (tp + fn)\nprint(\"Recall Scores: \", recall_score(Y_test, predict, average=None))\n\nprint(\"Random Forest Model Accuracy: \", accuracy_score(Y_test, predict))\n\nPrecision Scores:  [1. 1. 1. 1. 1.]\nRecall Scores:  [1. 1. 1. 1. 1.]\nRandom Forest Model Accuracy:  1.0\n\n\nFor every performance metric our model shows a 1.0, indicating it is 100% accurate to compared to the data set. This means that our model produces no false positives or false negatives. The Random Forest classifier fits the data very well.\nWe can also look at it side-by-side through a table, and see how the model predicted performance comapres to the actual performance.\n\n# Side by Side comparison\npd.DataFrame({\"Actual Performance: \" : Y_test[:10], \"Model Predicted Performance\" : predict[:10]})\n\n\n\n\n\n\n\n\nActual Performance:\nModel Predicted Performance\n\n\n\n\n95\ndrugX\ndrugX\n\n\n15\nDrugY\nDrugY\n\n\n30\ndrugX\ndrugX\n\n\n158\ndrugC\ndrugC\n\n\n128\nDrugY\nDrugY\n\n\n115\nDrugY\nDrugY\n\n\n69\nDrugY\nDrugY\n\n\n170\ndrugX\ndrugX\n\n\n174\ndrugA\ndrugA\n\n\n45\ndrugX\ndrugX\n\n\n\n\n\n\n\n\n\n\nNow that we see how well our model works, let’s look at which factors, or features, our Random Forest actually used to make decisions. As we noticed at the beginning, there were some features that appeared to heavily influence which drug was chosen.\nWe will use sklearn’s feature selection capabilities for this through the SelectFromModel option. This will give us the ability to look at what features were used.\n\n# Let's look at which features were the most important. Using select from model\nmodel = SelectFromModel(RandomForestClassifier(n_estimators=200, random_state=42))\nmodel.fit(X_train, Y_train)\nmodel.get_support()[:5]\n\narray([ True,  True, False, False,  True])\n\n\nThis tells us that 3 of the 5 features given were used by the model to perform the classification. This is useful, but we also want to know exactly which 3 features were used, and will have to display that.\n\n# See which features were used by Random Forest\nfeatures = X_train.columns[(model.get_support())]\nprint(features)\n\nIndex(['Age', 'Na_to_K', 'BP_HIGH'], dtype='object')\n\n\nThe 3 features used were Age, Sodium to Potassium ratio, and BP_HIGH. These 3 features had the biggest impact in decision making, so they were used by the classifier. As we analyzed above from the plots, we saw how Age and the Sodium to Potassium ratio did seem to affect which drug was administered, so this was expected.\nLet’s plot the distribution of our features importance, to see how important each was. We will set our nlargest to 3, so that our 3 main features will be displayed.\n\npd.Series(model.estimator_.feature_importances_, index=X.columns).nlargest(3).plot(kind='barh')\n\n&lt;Axes: &gt;\n\n\n\n\n\nIt appears that the Sodium to Potassium ratio was by far the most important factor in the model’s classification, when it comes to deciding which drug to administer."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n2"
  },
  {
    "objectID": "python-env-test/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "href": "python-env-test/lib/python3.12/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "posts/Probability_Blog_Post/Probability.html",
    "href": "posts/Probability_Blog_Post/Probability.html",
    "title": "Probability Blog",
    "section": "",
    "text": "This blog post looks at probability within Machine Learning, and how it is used with algorithms like Logistic Regression to analyze data sets. This blog will look at analyzing a voting patterns of two parties in the US Government’s House.\n\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Scikit learn (sklearn) libraries will also be imported for its metrics and models. We will use the metrics to see how accurate the model is and build a confusion matrix. The model libraries will be used to build the Logistic Regression and also split the data.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"house_votes.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nClass Name\nhandicapped-infants\nwater-project-cost-sharing\nadoption-of-the-budget-resolution\nphysician-fee-freeze\nel-salvador-aid\nreligious-groups-in-schools\nanti-satellite-test-ban\naid-to-nicaraguan-contras\nmx-missile\nimmigration\nsynfuels-corporation-cutback\neducation-spending\nsuperfund-right-to-sue\ncrime\nduty-free-exports\nexport-administration-act-south-africa\n\n\n\n\n0\nrepublican\nn\ny\nn\ny\ny\ny\nn\nn\nn\ny\n?\ny\ny\ny\nn\ny\n\n\n1\nrepublican\nn\ny\nn\ny\ny\ny\nn\nn\nn\nn\nn\ny\ny\ny\nn\n?\n\n\n2\ndemocrat\n?\ny\ny\n?\ny\ny\nn\nn\nn\nn\ny\nn\ny\ny\nn\nn\n\n\n3\ndemocrat\nn\ny\ny\nn\n?\ny\nn\nn\nn\nn\ny\nn\ny\nn\nn\ny\n\n\n4\ndemocrat\ny\ny\ny\nn\ny\ny\nn\nn\nn\nn\ny\n?\ny\ny\ny\ny\n\n\n\n\n\n\n\nThe dimensions of the data is (435, 17). There were 16 different issues for which voting in the House of Representatives was conducted, and 435 representatives voted. However, in the data set we can see some votes listed as ‘?’. This indicates that a vote was not given. We can now see how the data is structured and the way votes were provided.\n\n\n\nLet’s plot the data to visualize what the distributions look like and see if we can draw any initial inferences from what we see.\nFirst, let’s look at what the voting distribution actually looked like. While we have all the votes, it is difficult to see how many votes for yes/no/? were actually recevied in each category. Let’s first create a table to display this information.\n\n# Display data distribution\ndf = pd.DataFrame([], columns=['Yes', 'No', \"?\"])\nfor col in data.columns[1:]: # '1:' to skip first column (Class Name)\n    vals = []\n    yes = data[col].value_counts()['y']\n    no = data[col].value_counts()['n']\n    na = data[col].value_counts()['?']\n\n    vals.append(yes)\n    vals.append(no)\n    vals.append(na)\n\n    df.loc[col] = vals\n\n#Visualize df vote distribution\ndf\n\n\n\n\n\n\n\n\nYes\nNo\n?\n\n\n\n\nhandicapped-infants\n187\n236\n12\n\n\nwater-project-cost-sharing\n195\n192\n48\n\n\nadoption-of-the-budget-resolution\n253\n171\n11\n\n\nphysician-fee-freeze\n177\n247\n11\n\n\nel-salvador-aid\n212\n208\n15\n\n\nreligious-groups-in-schools\n272\n152\n11\n\n\nanti-satellite-test-ban\n239\n182\n14\n\n\naid-to-nicaraguan-contras\n242\n178\n15\n\n\nmx-missile\n207\n206\n22\n\n\nimmigration\n216\n212\n7\n\n\nsynfuels-corporation-cutback\n150\n264\n21\n\n\neducation-spending\n171\n233\n31\n\n\nsuperfund-right-to-sue\n209\n201\n25\n\n\ncrime\n248\n170\n17\n\n\nduty-free-exports\n174\n233\n28\n\n\nexport-administration-act-south-africa\n269\n62\n104\n\n\n\n\n\n\n\nThis is better to look at the overall voting spread of each category. After a first glance, it appears that most category had more ‘yes’ votes, indicating that in favor of the bill or issue passed was more likely.\nLet’s display this on a stacked bar graph to see how the spread looks on a plot. We will also store the vote result in a list. This will be used later when we want to compare the winners. The list will be in left to right order of the categories provided.\n\n# Plot data on stacked bar graph\ndf.plot.bar(stacked=True)\nwin_vote = ['No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes']\n\n\n\n\nLet’s look at party specific data now. We know that two parties, the Republicans and Democrats, were present in voting. Let’s see how many members from each party are present.\n\n# Look at party specific data, who voted yes/no in each category\nprint(\"Republicans: \", data['Class Name'].value_counts()['republican'])\nprint(\"Democrats: \", data['Class Name'].value_counts()['democrat'])\n\nlabels = ['Republicans', 'Democrats']\nplt.pie(data['Class Name'].value_counts(), labels=labels, autopct='%1.0f%%')\nplt.show()\n\nRepublicans:  168\nDemocrats:  267\n\n\n\n\n\nThere are more Democrats present than Republicans. This is important information as it could indicate one party had stronger voting power, which may have played a role in which party got its favored outcome for each bill passed.\nLet’s take each issue, and split it amongst the voting distribution for both parties. We can look at this and see which party got the favorable outcome. This will be done using the pivot table method which will build the table per each column against the Class Name. We will also use the win_votes list to add to the table as we go through the columns.\n\n# Find number of wins for each voting issue per party \ndf2 = pd.DataFrame(data)\ni = 0 # Go through winning votes\nfor col in data.columns[1:]: # 1: Skips class name\n    table = df2.pivot_table(index='Class Name', columns=col, aggfunc='size', fill_value=0)\n    table['Winning Vote'] = win_vote[i]\n    table.at['republican', 'Winning Vote'] = 'NA' #Format, remove from last\n    display(table)\n    i = i + 1\n\n\n\n\n\n\n\nhandicapped-infants\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n9\n102\n156\nNo\n\n\nrepublican\n3\n134\n31\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nwater-project-cost-sharing\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n28\n119\n120\nYes\n\n\nrepublican\n20\n73\n75\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nadoption-of-the-budget-resolution\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n7\n29\n231\nYes\n\n\nrepublican\n4\n142\n22\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nphysician-fee-freeze\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n8\n245\n14\nNo\n\n\nrepublican\n3\n2\n163\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nel-salvador-aid\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n12\n200\n55\nYes\n\n\nrepublican\n3\n8\n157\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nreligious-groups-in-schools\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n9\n135\n123\nYes\n\n\nrepublican\n2\n17\n149\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nanti-satellite-test-ban\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n8\n59\n200\nYes\n\n\nrepublican\n6\n123\n39\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\naid-to-nicaraguan-contras\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n4\n45\n218\nYes\n\n\nrepublican\n11\n133\n24\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nmx-missile\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n19\n60\n188\nYes\n\n\nrepublican\n3\n146\n19\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nimmigration\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n4\n139\n124\nYes\n\n\nrepublican\n3\n73\n92\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nsynfuels-corporation-cutback\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n12\n126\n129\nNo\n\n\nrepublican\n9\n138\n21\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\neducation-spending\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n18\n213\n36\nNo\n\n\nrepublican\n13\n20\n135\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nsuperfund-right-to-sue\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n15\n179\n73\nYes\n\n\nrepublican\n10\n22\n136\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrime\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n10\n167\n90\nYes\n\n\nrepublican\n7\n3\n158\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nduty-free-exports\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n16\n91\n160\nNo\n\n\nrepublican\n12\n142\n14\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nexport-administration-act-south-africa\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n82\n12\n173\nYes\n\n\nrepublican\n22\n50\n96\nNA\n\n\n\n\n\n\n\nFrom all the tables, we can see that democrats voting was generally much stronger, but partially also because there were more members of that party present. Most bills were voted in favor of (yes), and generally Democrats were more likely to get the favorable outcome.\n\n\n\nNow that we know what to expect from our analysis and how our data spread is structured, we can prepare the data set for the model.\nSince the data comes in yes/no format, we will need to convert this to numerical values so our Logistic Regression can work with the data. We will map ‘republican’ to 1 and ‘democrat’ to 0. Similarly for the data, we will map ‘yes’ to 1 and ‘no’ to 0. Since we do not want to count ‘?’ votes towards any single party, we will mark those votes with a value of 0.5, a neutral point.\n\n# Prep data for Logistic Regression\nX = data.copy()\nX['Class Name'] = X['Class Name'].map({'republican':1, 'democrat':0})\n\nfor col in X.columns.drop('Class Name'):\n    X[col] = X[col].map( \n                   {'y':1 ,'n':0, '?':0.5})\n\nprint(display(X.head()))\n\n\n\n\n\n\n\n\nClass Name\nhandicapped-infants\nwater-project-cost-sharing\nadoption-of-the-budget-resolution\nphysician-fee-freeze\nel-salvador-aid\nreligious-groups-in-schools\nanti-satellite-test-ban\naid-to-nicaraguan-contras\nmx-missile\nimmigration\nsynfuels-corporation-cutback\neducation-spending\nsuperfund-right-to-sue\ncrime\nduty-free-exports\nexport-administration-act-south-africa\n\n\n\n\n0\n1\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.5\n1.0\n1.0\n1.0\n0.0\n1.0\n\n\n1\n1\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.5\n\n\n2\n0\n0.5\n1.0\n1.0\n0.5\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n\n\n3\n0\n0.0\n1.0\n1.0\n0.0\n0.5\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n4\n0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.5\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\nNone\n\n\n\n\n\nFirst, the data will be split into training and testing sets, so we have can compare how well our model works after training it on the train set.\n\n# Split data set into train and test sets, use standard 80-20 split\nX_train, X_test, Y_train, Y_test = train_test_split(X.drop('Class Name',axis=1), X['Class Name'], train_size=0.8, test_size=0.2)\n\nWe will then build the Logistic Regression model. Since this model applies the Maximum Likelihood method, it is very powerful for calculating probabilities. It essentially will use these to figure out how to classify the data. This model will take the data and determine which party the individual belongs to based on their voting data. We will fit the data and then create a prediction list.\n\n# Split data set into train and test sets, use standard 80-20 split\nX_train, X_test, Y_train, Y_test = train_test_split(X.drop('Class Name',axis=1), X['Class Name'], train_size=0.8, test_size=0.2)\n\n# Logistic Regression\nlog = LogisticRegression()\nlog.fit(X_train, Y_train)\npredict = log.predict(X_test)\n\nLet’s check how well our model works by finding its accuracy score. We will compare the predict set to the test set.\n\nscore = accuracy_score(Y_test, predict)\nprint(score)\n\n0.9885057471264368\n\n\nThe model performs very well to this data set and has good accuracy. Over 90% accuracy means this Logistic Regression model fits the data set very strongly.\n\n\n\nTo better understand how our model classified the data and its overall accuracy, we will look at a confusion matrix. This will show how many predictions are correct and incorrect, essentially giving us the prediction summary. It will provide data on the True Negatives, False Positives, False Negatives, and True Positives.\n\nconfusion = confusion_matrix(Y_test, predict)\nconfusion\n\narray([[44,  1],\n       [ 0, 42]])\n\n\nThis is the output array. Let’s graph this to visualize it better, for which we can use the ConfusionMatrixDisplay.\n\n# Show confusion matrix on plot\ncm_display = ConfusionMatrixDisplay(confusion_matrix = confusion, display_labels = [False, True])\ncm_display.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x124eda510&gt;\n\n\n\n\n\nThis lets us visualize how the Confusion matrix appears."
  },
  {
    "objectID": "posts/Probability_Blog_Post/Probability.html#setup",
    "href": "posts/Probability_Blog_Post/Probability.html#setup",
    "title": "Probability Blog",
    "section": "",
    "text": "We will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, and matplotlib’s pyplot to display graphs and plot our data. Scikit learn (sklearn) libraries will also be imported for its metrics and models. We will use the metrics to see how accurate the model is and build a confusion matrix. The model libraries will be used to build the Logistic Regression and also split the data.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"house_votes.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nClass Name\nhandicapped-infants\nwater-project-cost-sharing\nadoption-of-the-budget-resolution\nphysician-fee-freeze\nel-salvador-aid\nreligious-groups-in-schools\nanti-satellite-test-ban\naid-to-nicaraguan-contras\nmx-missile\nimmigration\nsynfuels-corporation-cutback\neducation-spending\nsuperfund-right-to-sue\ncrime\nduty-free-exports\nexport-administration-act-south-africa\n\n\n\n\n0\nrepublican\nn\ny\nn\ny\ny\ny\nn\nn\nn\ny\n?\ny\ny\ny\nn\ny\n\n\n1\nrepublican\nn\ny\nn\ny\ny\ny\nn\nn\nn\nn\nn\ny\ny\ny\nn\n?\n\n\n2\ndemocrat\n?\ny\ny\n?\ny\ny\nn\nn\nn\nn\ny\nn\ny\ny\nn\nn\n\n\n3\ndemocrat\nn\ny\ny\nn\n?\ny\nn\nn\nn\nn\ny\nn\ny\nn\nn\ny\n\n\n4\ndemocrat\ny\ny\ny\nn\ny\ny\nn\nn\nn\nn\ny\n?\ny\ny\ny\ny\n\n\n\n\n\n\n\nThe dimensions of the data is (435, 17). There were 16 different issues for which voting in the House of Representatives was conducted, and 435 representatives voted. However, in the data set we can see some votes listed as ‘?’. This indicates that a vote was not given. We can now see how the data is structured and the way votes were provided.\n\n\n\nLet’s plot the data to visualize what the distributions look like and see if we can draw any initial inferences from what we see.\nFirst, let’s look at what the voting distribution actually looked like. While we have all the votes, it is difficult to see how many votes for yes/no/? were actually recevied in each category. Let’s first create a table to display this information.\n\n# Display data distribution\ndf = pd.DataFrame([], columns=['Yes', 'No', \"?\"])\nfor col in data.columns[1:]: # '1:' to skip first column (Class Name)\n    vals = []\n    yes = data[col].value_counts()['y']\n    no = data[col].value_counts()['n']\n    na = data[col].value_counts()['?']\n\n    vals.append(yes)\n    vals.append(no)\n    vals.append(na)\n\n    df.loc[col] = vals\n\n#Visualize df vote distribution\ndf\n\n\n\n\n\n\n\n\nYes\nNo\n?\n\n\n\n\nhandicapped-infants\n187\n236\n12\n\n\nwater-project-cost-sharing\n195\n192\n48\n\n\nadoption-of-the-budget-resolution\n253\n171\n11\n\n\nphysician-fee-freeze\n177\n247\n11\n\n\nel-salvador-aid\n212\n208\n15\n\n\nreligious-groups-in-schools\n272\n152\n11\n\n\nanti-satellite-test-ban\n239\n182\n14\n\n\naid-to-nicaraguan-contras\n242\n178\n15\n\n\nmx-missile\n207\n206\n22\n\n\nimmigration\n216\n212\n7\n\n\nsynfuels-corporation-cutback\n150\n264\n21\n\n\neducation-spending\n171\n233\n31\n\n\nsuperfund-right-to-sue\n209\n201\n25\n\n\ncrime\n248\n170\n17\n\n\nduty-free-exports\n174\n233\n28\n\n\nexport-administration-act-south-africa\n269\n62\n104\n\n\n\n\n\n\n\nThis is better to look at the overall voting spread of each category. After a first glance, it appears that most category had more ‘yes’ votes, indicating that in favor of the bill or issue passed was more likely.\nLet’s display this on a stacked bar graph to see how the spread looks on a plot. We will also store the vote result in a list. This will be used later when we want to compare the winners. The list will be in left to right order of the categories provided.\n\n# Plot data on stacked bar graph\ndf.plot.bar(stacked=True)\nwin_vote = ['No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes']\n\n\n\n\nLet’s look at party specific data now. We know that two parties, the Republicans and Democrats, were present in voting. Let’s see how many members from each party are present.\n\n# Look at party specific data, who voted yes/no in each category\nprint(\"Republicans: \", data['Class Name'].value_counts()['republican'])\nprint(\"Democrats: \", data['Class Name'].value_counts()['democrat'])\n\nlabels = ['Republicans', 'Democrats']\nplt.pie(data['Class Name'].value_counts(), labels=labels, autopct='%1.0f%%')\nplt.show()\n\nRepublicans:  168\nDemocrats:  267\n\n\n\n\n\nThere are more Democrats present than Republicans. This is important information as it could indicate one party had stronger voting power, which may have played a role in which party got its favored outcome for each bill passed.\nLet’s take each issue, and split it amongst the voting distribution for both parties. We can look at this and see which party got the favorable outcome. This will be done using the pivot table method which will build the table per each column against the Class Name. We will also use the win_votes list to add to the table as we go through the columns.\n\n# Find number of wins for each voting issue per party \ndf2 = pd.DataFrame(data)\ni = 0 # Go through winning votes\nfor col in data.columns[1:]: # 1: Skips class name\n    table = df2.pivot_table(index='Class Name', columns=col, aggfunc='size', fill_value=0)\n    table['Winning Vote'] = win_vote[i]\n    table.at['republican', 'Winning Vote'] = 'NA' #Format, remove from last\n    display(table)\n    i = i + 1\n\n\n\n\n\n\n\nhandicapped-infants\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n9\n102\n156\nNo\n\n\nrepublican\n3\n134\n31\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nwater-project-cost-sharing\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n28\n119\n120\nYes\n\n\nrepublican\n20\n73\n75\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nadoption-of-the-budget-resolution\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n7\n29\n231\nYes\n\n\nrepublican\n4\n142\n22\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nphysician-fee-freeze\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n8\n245\n14\nNo\n\n\nrepublican\n3\n2\n163\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nel-salvador-aid\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n12\n200\n55\nYes\n\n\nrepublican\n3\n8\n157\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nreligious-groups-in-schools\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n9\n135\n123\nYes\n\n\nrepublican\n2\n17\n149\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nanti-satellite-test-ban\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n8\n59\n200\nYes\n\n\nrepublican\n6\n123\n39\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\naid-to-nicaraguan-contras\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n4\n45\n218\nYes\n\n\nrepublican\n11\n133\n24\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nmx-missile\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n19\n60\n188\nYes\n\n\nrepublican\n3\n146\n19\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nimmigration\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n4\n139\n124\nYes\n\n\nrepublican\n3\n73\n92\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nsynfuels-corporation-cutback\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n12\n126\n129\nNo\n\n\nrepublican\n9\n138\n21\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\neducation-spending\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n18\n213\n36\nNo\n\n\nrepublican\n13\n20\n135\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nsuperfund-right-to-sue\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n15\n179\n73\nYes\n\n\nrepublican\n10\n22\n136\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrime\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n10\n167\n90\nYes\n\n\nrepublican\n7\n3\n158\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nduty-free-exports\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n16\n91\n160\nNo\n\n\nrepublican\n12\n142\n14\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nexport-administration-act-south-africa\n?\nn\ny\nWinning Vote\n\n\nClass Name\n\n\n\n\n\n\n\n\ndemocrat\n82\n12\n173\nYes\n\n\nrepublican\n22\n50\n96\nNA\n\n\n\n\n\n\n\nFrom all the tables, we can see that democrats voting was generally much stronger, but partially also because there were more members of that party present. Most bills were voted in favor of (yes), and generally Democrats were more likely to get the favorable outcome.\n\n\n\nNow that we know what to expect from our analysis and how our data spread is structured, we can prepare the data set for the model.\nSince the data comes in yes/no format, we will need to convert this to numerical values so our Logistic Regression can work with the data. We will map ‘republican’ to 1 and ‘democrat’ to 0. Similarly for the data, we will map ‘yes’ to 1 and ‘no’ to 0. Since we do not want to count ‘?’ votes towards any single party, we will mark those votes with a value of 0.5, a neutral point.\n\n# Prep data for Logistic Regression\nX = data.copy()\nX['Class Name'] = X['Class Name'].map({'republican':1, 'democrat':0})\n\nfor col in X.columns.drop('Class Name'):\n    X[col] = X[col].map( \n                   {'y':1 ,'n':0, '?':0.5})\n\nprint(display(X.head()))\n\n\n\n\n\n\n\n\nClass Name\nhandicapped-infants\nwater-project-cost-sharing\nadoption-of-the-budget-resolution\nphysician-fee-freeze\nel-salvador-aid\nreligious-groups-in-schools\nanti-satellite-test-ban\naid-to-nicaraguan-contras\nmx-missile\nimmigration\nsynfuels-corporation-cutback\neducation-spending\nsuperfund-right-to-sue\ncrime\nduty-free-exports\nexport-administration-act-south-africa\n\n\n\n\n0\n1\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.5\n1.0\n1.0\n1.0\n0.0\n1.0\n\n\n1\n1\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.5\n\n\n2\n0\n0.5\n1.0\n1.0\n0.5\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n\n\n3\n0\n0.0\n1.0\n1.0\n0.0\n0.5\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n4\n0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.5\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\nNone\n\n\n\n\n\nFirst, the data will be split into training and testing sets, so we have can compare how well our model works after training it on the train set.\n\n# Split data set into train and test sets, use standard 80-20 split\nX_train, X_test, Y_train, Y_test = train_test_split(X.drop('Class Name',axis=1), X['Class Name'], train_size=0.8, test_size=0.2)\n\nWe will then build the Logistic Regression model. Since this model applies the Maximum Likelihood method, it is very powerful for calculating probabilities. It essentially will use these to figure out how to classify the data. This model will take the data and determine which party the individual belongs to based on their voting data. We will fit the data and then create a prediction list.\n\n# Split data set into train and test sets, use standard 80-20 split\nX_train, X_test, Y_train, Y_test = train_test_split(X.drop('Class Name',axis=1), X['Class Name'], train_size=0.8, test_size=0.2)\n\n# Logistic Regression\nlog = LogisticRegression()\nlog.fit(X_train, Y_train)\npredict = log.predict(X_test)\n\nLet’s check how well our model works by finding its accuracy score. We will compare the predict set to the test set.\n\nscore = accuracy_score(Y_test, predict)\nprint(score)\n\n0.9885057471264368\n\n\nThe model performs very well to this data set and has good accuracy. Over 90% accuracy means this Logistic Regression model fits the data set very strongly.\n\n\n\nTo better understand how our model classified the data and its overall accuracy, we will look at a confusion matrix. This will show how many predictions are correct and incorrect, essentially giving us the prediction summary. It will provide data on the True Negatives, False Positives, False Negatives, and True Positives.\n\nconfusion = confusion_matrix(Y_test, predict)\nconfusion\n\narray([[44,  1],\n       [ 0, 42]])\n\n\nThis is the output array. Let’s graph this to visualize it better, for which we can use the ConfusionMatrixDisplay.\n\n# Show confusion matrix on plot\ncm_display = ConfusionMatrixDisplay(confusion_matrix = confusion, display_labels = [False, True])\ncm_display.plot()\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x124eda510&gt;\n\n\n\n\n\nThis lets us visualize how the Confusion matrix appears."
  },
  {
    "objectID": "posts/LinReg_Blog_Post/LinReg.html",
    "href": "posts/LinReg_Blog_Post/LinReg.html",
    "title": "Linear Regression Blog",
    "section": "",
    "text": "This blog post will cover data analysis with multiple Linear Regression. The data set holds information on the performance of students along with their sleep hours, study hours, previous scores, practiced questions, and extracurricular participation. The goal here, is to identify how these factors affect a student’s performance index. To do this, a multiple Linear Regression will be performed on the data set to analyze whether a correlation does in fact exist.\n\n\nWe will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, numpy for array building, matplotlib’s pyplot to display graphs and plot our data. Scikit learn (sklearn) libraries will also be imported to help perform the Linear Regression.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"Student_Performance.csv\")\nprint(data.shape)\ndata.head()\n\n(10000, 6)\n\n\n\n\n\n\n\n\n\nHours Studied\nPrevious Scores\nExtracurricular Activities\nSleep Hours\nSample Question Papers Practiced\nPerformance Index\n\n\n\n\n0\n7\n99\nYes\n9\n1\n91.0\n\n\n1\n4\n82\nNo\n4\n2\n65.0\n\n\n2\n8\n51\nYes\n7\n2\n45.0\n\n\n3\n5\n52\nYes\n5\n2\n36.0\n\n\n4\n7\n75\nNo\n8\n5\n66.0\n\n\n\n\n\n\n\nThe dimensions of the data is (10000, 6), which means that the data has 6 columns: Hours Studied, Previous Scores, Extracurricular Activities, Sleep Hours, Sample Question Papers Practiced, and Performance Index. These 6 categories are provided for each student, and there are 10,000 student entries present in the data set. We want to see how the first 5 factors affect a student’s Performance Index.\n\n\n\nTo understand and visualize how each factor individually affects student Performance Index, we will plot the data for each factor.\n\n\n\n\nplt.xlabel('Hours Studied')\nplt.ylabel('Performance')\nplt.scatter(data['Hours Studied'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x12fee1750&gt;\n\n\n\n\n\nHours Studied does seem to have a correlation to a student’s Performance Index. A general trend that appears to be present is that the longer a student studies, the more likely they are to perform better.\n\nplt.xlabel('Previous Scores')\nplt.ylabel('Performance')\nplt.scatter(data['Previous Scores'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x12ff6fa90&gt;\n\n\n\n\n\nPrevious scores appear to have a significant correlation to Performance Index. The better the student’s score previously, the more likely they are to have higher performance.\n\nlabels = ['no', 'yes']\nplt.pie(data['Extracurricular Activities'].value_counts(), labels=labels, autopct='%1.0f%%')\nplt.show()\n\n\n\n\nThis shows us the number of student’s participating in Extracurricular Activities. Because the data is provided in Yes or No format, a pie chart makes the most sense to visualize this data. The numbers of students who participate and do not are roughly about the same, with slightly more student not participating in Extracurricular Activites.\n\nplt.xlabel('Sleep Hours')\nplt.ylabel('Performance')\nplt.scatter(data['Sleep Hours'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x12fffcc90&gt;\n\n\n\n\n\nThe data is spread out fairly similarly. This may indicate that the number of hours slept does not have a significant impact on a student’s performance.\n\nplt.xlabel('Sample Questions Practices')\nplt.ylabel('Performance')\nplt.scatter(data['Sample Question Papers Practiced'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x130074690&gt;\n\n\n\n\n\nLooking at the Sample Questions Practiced, it also appears as if the data is about evenly spread out, indicating that it may not have much of an impact on student performance.\n\n\n\nBefore we dive into the Linear Regression, we will have to prepare our data and also split the data set into its respective training and testing sets. For splitting the data, we will use the standard 80-20 split, where 80% of our data will be used to train our multiple Linear Regression model, and the remaining 20% will be used to test the model.\n\n# Prep model, split data into training and testing groups (follow standard 80-20 split)\n# Change the yes/no to 1/0 to fit Linear Regression\ndata['Extracurricular Activities'].replace(('Yes', 'No'), (1, 0), inplace=True)\nX = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']]\nY = data['Performance Index']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=42)\nprint(\"X-Training Set Dimensions: \", X_train.shape)\nprint(\"X-Test Set Dimensions: \", X_test.shape)\nprint(\"Y-Training Set Dimensions: \", Y_train.shape)\nprint(\"Y-Test Set Dimensions: \", Y_test.shape)\n\nX-Training Set Dimensions:  (8000, 5)\nX-Test Set Dimensions:  (2000, 5)\nY-Training Set Dimensions:  (8000,)\nY-Test Set Dimensions:  (2000,)\n\n\nNow that our data has been fixed and placed into its respective groups as expected, we can begin with the performing the Linear Regression.\n\n# Create Linear Regression Model\nreg = LinearRegression()\nreg.fit(X_train, Y_train)\npredict = reg.predict(X_test)\n\nThe sklearn library allows us to conduct a Linear Regression. We will fit the data to the model so that it can perform the Linear Regression, and then use the predict functionality on our test set to get a set of the predicted values.\n\n\n\n\n# Check model accuracy to determine how well it fits the actual performance\nreg.score(X_test, Y_test)\n\n0.9889832909573145\n\n\nThe score functionality is similar to producing an r-value to find correlation. The value we have, which is roughly 0.989, indicates our model is very accurate, and that a strong correlation exists between the model predicted values and the actual values.\nWe can plot the predicted value against the actual values to better visualize how well the model fits the data set.\n\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs Predicted Values')\n\nplt.scatter(Y_test, predict, color='red', marker='+')\narr = np.array(Y_test)\narr2 = np.array(predict)\nplt.plot([10, 20, 40, 60, 80, 100], [10, 20, 40, 60, 80, 100], \"b-\", label=\"Predictions\")\nplt.show()\n\n\n\n\nLet’s also take a look at this in table format using the Pandas DataFrame functionality, which will allow us to get a side-by-side comparison of the results.\n\npd.DataFrame({\"Actual Performance: \" : Y_test[:10], \"Model Predicted Performance\" : predict[:10]})\n\n\n\n\n\n\n\n\nActual Performance:\nModel Predicted Performance\n\n\n\n\n6252\n51.0\n54.711854\n\n\n4684\n20.0\n22.615513\n\n\n1731\n46.0\n47.903145\n\n\n4742\n28.0\n31.289767\n\n\n4521\n41.0\n43.004570\n\n\n6340\n59.0\n59.071252\n\n\n576\n48.0\n45.903475\n\n\n5202\n87.0\n86.459118\n\n\n6363\n37.0\n37.700140\n\n\n439\n73.0\n72.055925\n\n\n\n\n\n\n\nThe predicted values are very close to the actual values, and we see in the table how some differ more than others. In the first row, the two values differ by about 3 points, but in row 6 the values differ by only about 0.7 points.\nNow, let’s find out what our linear equation actually is for this model we built. To do this, we will have to find the coefficients the model created for each factor, as well as our y-intercept.\n\nprint(\"Coefficients: \", reg.coef_)\nprint(\"Y-intercept: \", reg.intercept_)\n\nCoefficients:  [2.85248393 1.0169882  0.60861668 0.47694148 0.19183144]\nY-intercept:  -33.92194621555529\n\n\nThe coefficients are listed in the order the data was passed in, and from this information we can get our equation. The Linear Regression formula is:\n2.85(Hours Studied) + 1.02(Previous Scores) + 0.61(Extracurricular Activities) + 0.48(Sleep Hours) + 0.19(Sample Questions) - 33.92"
  },
  {
    "objectID": "posts/LinReg_Blog_Post/LinReg.html#setup",
    "href": "posts/LinReg_Blog_Post/LinReg.html#setup",
    "title": "Linear Regression Blog",
    "section": "",
    "text": "We will first begin by checking our python version and importing the necessary libraries for this. We will use Pandas to read the csv file and manipulate its data, numpy for array building, matplotlib’s pyplot to display graphs and plot our data. Scikit learn (sklearn) libraries will also be imported to help perform the Linear Regression.\n\nimport sys\n\n#Project requires Python 3.7 or above\nassert sys.version_info &gt;= (3, 7)\n\n# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\n\nLet’s start by seeing what our data looks like.\n\n# Read data\ndata = pd.read_csv(\"Student_Performance.csv\")\nprint(data.shape)\ndata.head()\n\n(10000, 6)\n\n\n\n\n\n\n\n\n\nHours Studied\nPrevious Scores\nExtracurricular Activities\nSleep Hours\nSample Question Papers Practiced\nPerformance Index\n\n\n\n\n0\n7\n99\nYes\n9\n1\n91.0\n\n\n1\n4\n82\nNo\n4\n2\n65.0\n\n\n2\n8\n51\nYes\n7\n2\n45.0\n\n\n3\n5\n52\nYes\n5\n2\n36.0\n\n\n4\n7\n75\nNo\n8\n5\n66.0\n\n\n\n\n\n\n\nThe dimensions of the data is (10000, 6), which means that the data has 6 columns: Hours Studied, Previous Scores, Extracurricular Activities, Sleep Hours, Sample Question Papers Practiced, and Performance Index. These 6 categories are provided for each student, and there are 10,000 student entries present in the data set. We want to see how the first 5 factors affect a student’s Performance Index.\n\n\n\nTo understand and visualize how each factor individually affects student Performance Index, we will plot the data for each factor.\n\n\n\n\nplt.xlabel('Hours Studied')\nplt.ylabel('Performance')\nplt.scatter(data['Hours Studied'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x12fee1750&gt;\n\n\n\n\n\nHours Studied does seem to have a correlation to a student’s Performance Index. A general trend that appears to be present is that the longer a student studies, the more likely they are to perform better.\n\nplt.xlabel('Previous Scores')\nplt.ylabel('Performance')\nplt.scatter(data['Previous Scores'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x12ff6fa90&gt;\n\n\n\n\n\nPrevious scores appear to have a significant correlation to Performance Index. The better the student’s score previously, the more likely they are to have higher performance.\n\nlabels = ['no', 'yes']\nplt.pie(data['Extracurricular Activities'].value_counts(), labels=labels, autopct='%1.0f%%')\nplt.show()\n\n\n\n\nThis shows us the number of student’s participating in Extracurricular Activities. Because the data is provided in Yes or No format, a pie chart makes the most sense to visualize this data. The numbers of students who participate and do not are roughly about the same, with slightly more student not participating in Extracurricular Activites.\n\nplt.xlabel('Sleep Hours')\nplt.ylabel('Performance')\nplt.scatter(data['Sleep Hours'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x12fffcc90&gt;\n\n\n\n\n\nThe data is spread out fairly similarly. This may indicate that the number of hours slept does not have a significant impact on a student’s performance.\n\nplt.xlabel('Sample Questions Practices')\nplt.ylabel('Performance')\nplt.scatter(data['Sample Question Papers Practiced'], data['Performance Index'], color='red', marker='+')\n\n&lt;matplotlib.collections.PathCollection at 0x130074690&gt;\n\n\n\n\n\nLooking at the Sample Questions Practiced, it also appears as if the data is about evenly spread out, indicating that it may not have much of an impact on student performance.\n\n\n\nBefore we dive into the Linear Regression, we will have to prepare our data and also split the data set into its respective training and testing sets. For splitting the data, we will use the standard 80-20 split, where 80% of our data will be used to train our multiple Linear Regression model, and the remaining 20% will be used to test the model.\n\n# Prep model, split data into training and testing groups (follow standard 80-20 split)\n# Change the yes/no to 1/0 to fit Linear Regression\ndata['Extracurricular Activities'].replace(('Yes', 'No'), (1, 0), inplace=True)\nX = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']]\nY = data['Performance Index']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=42)\nprint(\"X-Training Set Dimensions: \", X_train.shape)\nprint(\"X-Test Set Dimensions: \", X_test.shape)\nprint(\"Y-Training Set Dimensions: \", Y_train.shape)\nprint(\"Y-Test Set Dimensions: \", Y_test.shape)\n\nX-Training Set Dimensions:  (8000, 5)\nX-Test Set Dimensions:  (2000, 5)\nY-Training Set Dimensions:  (8000,)\nY-Test Set Dimensions:  (2000,)\n\n\nNow that our data has been fixed and placed into its respective groups as expected, we can begin with the performing the Linear Regression.\n\n# Create Linear Regression Model\nreg = LinearRegression()\nreg.fit(X_train, Y_train)\npredict = reg.predict(X_test)\n\nThe sklearn library allows us to conduct a Linear Regression. We will fit the data to the model so that it can perform the Linear Regression, and then use the predict functionality on our test set to get a set of the predicted values.\n\n\n\n\n# Check model accuracy to determine how well it fits the actual performance\nreg.score(X_test, Y_test)\n\n0.9889832909573145\n\n\nThe score functionality is similar to producing an r-value to find correlation. The value we have, which is roughly 0.989, indicates our model is very accurate, and that a strong correlation exists between the model predicted values and the actual values.\nWe can plot the predicted value against the actual values to better visualize how well the model fits the data set.\n\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs Predicted Values')\n\nplt.scatter(Y_test, predict, color='red', marker='+')\narr = np.array(Y_test)\narr2 = np.array(predict)\nplt.plot([10, 20, 40, 60, 80, 100], [10, 20, 40, 60, 80, 100], \"b-\", label=\"Predictions\")\nplt.show()\n\n\n\n\nLet’s also take a look at this in table format using the Pandas DataFrame functionality, which will allow us to get a side-by-side comparison of the results.\n\npd.DataFrame({\"Actual Performance: \" : Y_test[:10], \"Model Predicted Performance\" : predict[:10]})\n\n\n\n\n\n\n\n\nActual Performance:\nModel Predicted Performance\n\n\n\n\n6252\n51.0\n54.711854\n\n\n4684\n20.0\n22.615513\n\n\n1731\n46.0\n47.903145\n\n\n4742\n28.0\n31.289767\n\n\n4521\n41.0\n43.004570\n\n\n6340\n59.0\n59.071252\n\n\n576\n48.0\n45.903475\n\n\n5202\n87.0\n86.459118\n\n\n6363\n37.0\n37.700140\n\n\n439\n73.0\n72.055925\n\n\n\n\n\n\n\nThe predicted values are very close to the actual values, and we see in the table how some differ more than others. In the first row, the two values differ by about 3 points, but in row 6 the values differ by only about 0.7 points.\nNow, let’s find out what our linear equation actually is for this model we built. To do this, we will have to find the coefficients the model created for each factor, as well as our y-intercept.\n\nprint(\"Coefficients: \", reg.coef_)\nprint(\"Y-intercept: \", reg.intercept_)\n\nCoefficients:  [2.85248393 1.0169882  0.60861668 0.47694148 0.19183144]\nY-intercept:  -33.92194621555529\n\n\nThe coefficients are listed in the order the data was passed in, and from this information we can get our equation. The Linear Regression formula is:\n2.85(Hours Studied) + 1.02(Previous Scores) + 0.61(Extracurricular Activities) + 0.48(Sleep Hours) + 0.19(Sample Questions) - 33.92"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "This Blog site will have many different blogs on it, each discussing a different Machine Learning topic."
  }
]